{"status":"ok","feed":{"url":"https://medium.com/feed/@bamarasingam","title":"Stories by Brandon Amarasingam on Medium","link":"https://medium.com/@bamarasingam?source=rss-ce7453a5c978------2","author":"","description":"Stories by Brandon Amarasingam on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*opjnih27-l-40LZqxpSNZA.png"},"items":[{"title":"Building a Crypto / Binance Dashboard with Machine Learning Predictions using Python &amp; Streamlit","pubDate":"2024-07-24 18:24:23","link":"https://medium.com/pythons-gurus/building-a-crypto-binance-dashboard-with-machine-learning-predictions-using-python-streamlit-5726dc964423?source=rss-ce7453a5c978------2","guid":"https://medium.com/p/5726dc964423","author":"Brandon Amarasingam","thumbnail":"","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*j_oah2QisrHvRGG5vFB4tA.gif\"><figcaption>Homepage for Streamlit App</figcaption></figure><p>In the fast-paced world of cryptocurrency markets, having access to real-time data and predictive insights can make all the difference. Today, I\u2019m excited to share with you a project that combines the power of data analytics with machine learning: a Crypto Technical Analysis Dashboard. This tool, built using Python and Streamlit, taps into Binance\u2019s API to provide traders with a comprehensive view of the crypto market. From interactive price charts and volume analysis to advanced machine learning predictions, this dashboard is designed to be your all-in-one crypto analysis companion. Whether you\u2019re a seasoned trader looking to refine your strategy or a curious developer interested in the intersection of finance and technology, this article will walk you through the creation of a powerful tool that brings together technical indicators, data visualization, and predictive modelling. Join me as we explore how to leverage Linear Regression, Logistic Regression, and Random Forest algorithms to forecast cryptocurrency price movements, all within an intuitive, user-friendly interface.</p>\n<p><em>Please note that while our predictions are based on sophisticated algorithms, cryptocurrency markets are highly volatile, and all predictions are purely for educational/entertainment purposes.</em></p>\n<p><strong>Part 1</strong> will focus on the <strong>Functions</strong> which were built to be used for the Streamlit app.</p>\n<p><strong>Part 2</strong> will focus on the <strong>Technical Analysis Dashboard</strong>, which includes the various technical indicators and analysis.</p>\n<p><strong>Part 3</strong> will focus on the various <strong>Machine Learning Algorithms</strong>, including linear and logistic regression, as well as a couple random forest\u00a0algos.</p>\n<p>Below, I will include all the necessary imports needed for this project and a link to the github repo. Do note, all associated files can be found within the \u2018medium\u2019\u00a0folder.</p>\n<p><a href=\"https://github.com/bamarasingam/binance_dashboard\">GitHub - bamarasingam/binance_dashboard</a></p>\n<pre>#Libraries<br>import streamlit as st<br>import pandas as pd<br>import pandas_ta as ta<br>import numpy as np<br>import matplotlib.pyplot as plt<br>import seaborn as sns<br>import plotly.graph_objs as go<br>from plotly.subplots import make_subplots<br><br>from binance.client import Client<br>from datetime import datetime, timedelta<br><br>from sklearn.linear_model import LinearRegression<br>from sklearn.linear_model import LogisticRegression<br>from sklearn.ensemble import RandomForestRegressor<br>from sklearn.ensemble import RandomForestClassifier<br><br>from sklearn.model_selection import train_test_split<br>from sklearn import metrics</pre>\n<p><strong>Part 1: Functions</strong></p>\n<p>We will explore a series of Python functions I\u2019ve developed to analyze financial data from Binance and generate insightful visualizations. These functions include data loading, emoji-based indicators, chart creation, and lag analysis. Each function plays a crucial role in simplifying and enhancing the process of financial data analysis. I will skip over some functions\u2019 explanation if they are redundant. Here\u2019s a brief overview of each function:</p>\n<ul><li>\n<strong>load_data(symbol, interval, start_date)</strong>:<em> </em>This function initializes a connection to Binance, retrieves historical candlestick data for a specified symbol and interval, and calculates several technical indicators. The data is then organized into a pandas DataFrame.</li></ul>\n<pre>#Function to load in data from Binance<br>def load_data(symbol, interval, start_date):<br>    #Initalize Binance client<br>    client = Client()<br><br>    #Convert start_date to ms timestamp<br>    start_datetime = datetime.combine(start_date, datetime.min.time())<br>    start_ts = int(start_datetime.timestamp() * 1000)<br><br>    #Fetch candlestick data<br>    candles = client.get_historical_klines(symbol, interval, start_ts)<br><br>    #Create df<br>    df = pd.DataFrame(candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', <br>                                       'close_time', 'quote_asset_volume', 'number_of_trades',<br>                                       'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])<br><br>    #Convert to datetime<br>    df['time'] = pd.to_datetime(df['timestamp'], unit='ms')<br><br>    #Keep OHLCV + time<br>    df = df[['time', 'open', 'high', 'low', 'close', 'volume']]<br><br>    #Convert values to float (other than time)<br>    for col in ['open', 'high', 'low', 'close', 'volume']:<br>        df[col] = df[col].astype(float)<br><br>    #Format time as string value<br>    #df['time'] = df['time'].dt.strftime('%Y-%m-%d')<br>    df['time'] = pd.to_datetime(df['time'])<br>    df.set_index('time', inplace=True)<br><br>    #Calculate indicators<br>    df.ta.ema(length=20, append=True)<br>    df.ta.ema(length=200, append=True)<br>    df.ta.rsi(length=14, append=True)<br>    df.ta.adx(length=14, append=True)<br>    df.ta.atr(length=14, append=True)<br>    df.ta.bbands(length=14, append=True)<br>    df.ta.macd(length=20,append = True)<br><br>    return df</pre>\n<ul><li>\n<strong>get_returns_emoji(ret_val)</strong>: This function returns a green checkmark emoji if the return value is non-negative and a red circle emoji if it is negative (Other emoji based functions are similar in implementation and reasoning).</li></ul>\n<pre>#Function to get emoji based on returns value<br>def get_returns_emoji(ret_val):<br>    return \":white_check_mark:\" if ret_val &gt;= 0 else \":red_circle:\"</pre>\n<ul><li>\n<strong>create_chart(df, symbol, chart_type)</strong>: This function creates a chart (either candlestick or line) with subplots for price and volume, and includes EMA lines for additional context. The resulting plotly figure is well-structured and visually informative.</li></ul>\n<pre>#Function to create chart<br>def create_chart(df, symbol, chart_type):<br>    #Create subplots<br>    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, <br>                        vertical_spacing=0.1, <br>                        subplot_titles=(f'{symbol} {chart_type} Chart', 'Volume'),<br>                        row_heights=[0.7, 0.3])<br><br>    #Add price chart<br>    if chart_type == \"Candlestick\":<br>        fig.add_trace(<br>            go.Candlestick(x=df.index,<br>                           open=df['open'],<br>                           high=df['high'],<br>                           low=df['low'],<br>                           close=df['close'],<br>                           name=\"Price\"),<br>            row=1, col=1<br>        )<br>    else:  #Line chart<br>        fig.add_trace(<br>            go.Scatter(x=df.index,<br>                       y=df['close'],<br>                       mode='lines',<br>                       name=\"Price\"),<br>            row=1, col=1<br>        )<br><br>    #Add EMA lines<br>    fig.add_trace(<br>        go.Scatter(x=df.index, y=df.EMA_20.values, name='EMA20', line=dict(color='blue')),<br>        row=1, col=1<br>    )<br>    fig.add_trace(<br>        go.Scatter(x=df.index, y=df.EMA_200.values, name='EMA200', line=dict(color='red')),<br>        row=1, col=1<br>    )<br><br>    #Add volume<br>    fig.add_trace(<br>        go.Bar(x=df.index, y=df['volume'], name='Volume'),<br>        row=2, col=1<br>    )<br><br>    #Update layout<br>    fig.update_layout(<br>        title_text=f'{symbol} Historical Data',<br>        xaxis_rangeslider_visible=False,<br>        height=800,  #Increase overall height of the figure<br>        showlegend=True<br>    )<br><br>    #Update y-axis<br>    fig.update_yaxes(title_text=\"Price\", row=1, col=1)<br>    fig.update_yaxes(title_text=\"Volume\", row=2, col=1)<br>    #Update x-axis<br>    fig.update_xaxes(<br>        rangeselector=dict(<br>            buttons=list([<br>                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),<br>                dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),<br>                dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),<br>                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),<br>                dict(step=\"all\")<br>            ])<br>        ),<br>        row=2, col=1  #Add range selector to bottom subplot<br>    )<br><br>    return fig</pre>\n<ul>\n<li>\n<strong>lagit(df, lags)</strong>: This function generates lagged returns columns for a specified number of lags, to aid in time series analysis.</li>\n<li>\n<strong>lagit_dir(df, lags)</strong>: This function generates lagged returns columns for a specified number of lags, aiding in time series analysis. Also includes directional indicators (1 for positive returns, 0 for negative returns) for the lagged\u00a0returns.</li>\n</ul>\n<pre>#Function for lags using returns<br>def lagit(df, lags):<br>    names = []<br>    for i in range(1, lags+1):<br>        df['Lag_' + str(i)] = df['returns'].shift(i)<br>        names.append('Lag_' + str(i))<br>    return names<br><br>#Updated lag function which includes lag direction<br>def lagit_dir(df, lags):<br>    names = []<br>    for i in range(1, lags+1):<br>        df['Lag_'+str(i)] = df['returns'].shift(i)<br>        df['Lag_'+str(i)+'_dir'] = [1 if j&gt;0 else 0 for j in df['Lag_'+str(i)]]<br>        names.append('Lag_'+str(i)+'_dir')<br>    return names</pre>\n<ul><li>\n<strong>color_text(val)</strong>: This function assigns a color to specific words (e.g., 'Increase', 'Buy', 'Decrease', 'Sell') for better visualization, using green for positive actions and red for negative\u00a0ones.</li></ul>\n<pre>#Colors for trigger words<br>def color_text(val):<br>    if val in ['Increase', 'Buy']:<br>        return 'color: green'<br>    elif val in ['Decrease', 'Sell']:<br>        return 'color: red'<br>    return ''</pre>\n<p><strong>Part 2: Technical Analysis Dashboard</strong></p>\n<p>In this section, we\u2019ll create a technical analysis dashboard to analyze cryptocurrency data from Binance. Users can input a cryptocurrency symbol, select data intervals, choose a start date, and pick a chart type. The data is loaded and processed to extract key metrics like closing prices, EMA, RSI, and ADX, etc. These metrics, along with the other features will help provide clear insights into market trends and performance.</p>\n<p>First, we will create our various tabs within the Streamlit dashboard, each designated for its own\u00a0purpose.</p>\n<pre>#Create tabs for dashboard<br>tab1, tab2, tab3, tab4, tab5 = st.tabs([\"Data/Analytics\", \"Lin Regression\", \"Log Regression\", \"RF Regressor\", \"RF Classification\"])</pre>\n<p>Once completed, we will initialize our first tab and center the\u00a0title.</p>\n<pre>with tab1:<br>    #Centered title<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Technical Analysis Dashboard&lt;/h2&gt;\", unsafe_allow_html=True) #Center title</pre>\n<p>Next, we add interactive sidebar components for user input. These include the cryptocurrency ticker symbol, time interval, start date, and chart type. Additionally, an informational note is displayed in the\u00a0sidebar.</p>\n<pre>#Sidebar Components<br>    symbol = st.sidebar.text_input(\"Crypto Symbol (ex. BTCUSDT)\\n\\nMust be a ticker from Binance\", \"BTCUSDT\")<br>    intervals = ('1m', '3m', '5m', '15m', '30m', '1h', '2h', '4h', '6h', '8h', '12h', '1d', '3d', '1w', '1M')<br>    default_index = intervals.index('1d')<br>    interval = st.sidebar.selectbox(\"Interval\", <br>                                    options=intervals,<br>                                    index=default_index)<br><br>    #Date input for start date<br>    default_date = datetime.now().date() - timedelta(days=365)<br>    start_date = st.sidebar.date_input(\"Start Date\", <br>                                    value= default_date,<br>                                    max_value=datetime.now())<br>    chart_type = st.sidebar.radio(\"Chart Type\", (\"Candlestick\", \"Line\"))<br><br>    st.sidebar.info(\"Note: The latest row is within a timeframe which has not completed. The close price is the price at the time at which you pulled the data for this most recent row.\")</pre>\n<p>Now it\u2019s time to load and preprocess our data. The load_data function is called to retrieve and process the data. The DataFrame is reversed to show the most recent data first. Key values such as closing price, EMA, RSI, ADX, and directional movement are extracted from the first row of the reversed DataFrame (to be used later). We will also need to calculate the returns percentages over different time periods, based on the closing prices at those intervals.</p>\n<pre>df = load_data(symbol, interval, start_date)<br>reversed_df = df.iloc[::-1] #Reversed dataframe to be shown in Streamlit<br>row1_val = reversed_df.iloc[0]['close']<br>ema20_val = reversed_df.iloc[0]['EMA_20']<br>ema200_val = reversed_df.iloc[0]['EMA_200']<br>rsi_val = reversed_df.iloc[0]['RSI_14']<br>adx = reversed_df.iloc[0]['ADX_14']<br>dmp = reversed_df.iloc[0]['DMP_14']<br>dmn = reversed_df.iloc[0]['DMN_14']<br><br>row20_val = reversed_df.iloc[20]['close'] if len(reversed_df) &gt; 20 else row1_val<br>row60_val = reversed_df.iloc[60]['close'] if len(reversed_df) &gt; 60 else row1_val<br>row120_val = reversed_df.iloc[120]['close'] if len(reversed_df) &gt; 120 else row1_val<br>row240_val = reversed_df.iloc[240]['close'] if len(reversed_df) &gt; 240 else row1_val<br><br>day20_ret_percent = (row1_val - row20_val)/row20_val * 100<br>day60_ret_percent = (row1_val - row60_val)/row60_val * 100<br>day120_ret_percent = (row1_val - row120_val)/row120_val * 100<br>day240_ret_percent = (row1_val - row240_val)/row240_val * 100</pre>\n<p>Once calculated, we can now display the above data. This below code uses Streamlit columns to display the data in a structured format. Each column contains specific metrics: returns, momentum indicators, and trend strength indicators, along with corresponding emojis for visual cues (remember we built these functions earlier).</p>\n<pre>#Displays (column wide)<br>col1, col2, col3 = st.columns(3)<br>with col1:<br>    st.subheader(\"Returns\")<br>    st.markdown(f\"- 1 MONTH : {round(day20_ret_percent,2)}% {get_returns_emoji(round(day20_ret_percent,2))}\")<br>    st.markdown(f\"- 3 MONTHS : {round(day60_ret_percent,2)}% {get_returns_emoji(round(day60_ret_percent,2))}\")<br>    st.markdown(f\"- 6 MONTHS : {round(day120_ret_percent,2)}% {get_returns_emoji(round(day120_ret_percent,2))}\")<br>    st.markdown(f\"- 12 MONTHS : {round(day240_ret_percent,2)}% {get_returns_emoji(round(day240_ret_percent,2))}\")<br>with col2:<br>    st.subheader(\"Momentum\")<br>    st.markdown(f\"- LTP : {round(row1_val,2)}\")<br>    st.markdown(f\"- EMA20 : {round(ema20_val,2)} {get_ema_emoji(round(row1_val,2),round(ema20_val,2))}\") <br>    st.markdown(f\"- EMA200 : {round(ema200_val,2)} {get_ema_emoji(round(row1_val,2),round(ema200_val,2))}\") <br>    st.markdown(f\"- RSI : {round(rsi_val,2)} {get_rsi_emoji(round(rsi_val,2))}\") <br>with col3:<br>    st.subheader(\"Trend Strength\")<br>    st.markdown(f\"- ADX : {round(adx,2)} {get_adx_emoji(round(adx,2))}\") <br>    st.markdown(f\"- DMP : {round(dmp,2)} \") <br>    st.markdown(f\"- DMN : {round(dmn,2)} \")</pre>\n<p>Finally, we will call the create_chart function to create and display our price chart. The chart, along with the reversed DataFrame, is shown in the Streamlit app, allowing for interactive data visualization.</p>\n<pre>#Display chart <br>st.plotly_chart(create_chart(df, symbol, chart_type), use_container_width=True)<br>#Display dataframe<br>st.write(reversed_df)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/proxy/1*j_oah2QisrHvRGG5vFB4tA.gif\"><figcaption>Technical Analysis Dashboard</figcaption></figure><p><strong>Part 3: Machine Learning Algorithms</strong></p>\n<p>In the final section, we will explore the implementation of various machine learning algorithms to predict cryptocurrency prices and trends using historical data. Each tab in our dashboard is dedicated to a specific algorithm: Linear Regression, Logistic Regression, Random Forest Regressor, and Random Forest Classifier. We will go through the process of setting up each model, training and testing it, and evaluating its performance. Due to the length of this process, I will go through the process in bullet points and skip over redundant parts. Let\u2019s dive into the details of each part of the\u00a0code.</p>\n<p><strong><em>Linear Regression Tab</em></strong></p>\n<ul>\n<li>Initialize our linear regression tab with centred\u00a0title</li>\n<li>Calculate logarithmic returns for our returns column, then call our lagit(df, lags) to create our lagged returns columns, and display the DataFrame within Streamlit</li>\n</ul>\n<pre>#Linear Regression Tab<br>with tab2:<br>    #Centered title<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Predictions Using Linear Regression&lt;/h2&gt;\", unsafe_allow_html=True)<br><br>    #Create returns column<br>    df['returns'] = np.log(df.close.pct_change() + 1)<br><br>    #Call function with associated amount of lags<br>    lagnames = lagit(df, 5)<br>    df.dropna(inplace=True)<br><br>    #Display the DataFrame with calculated lags<br>    st.subheader('Data with Calculated Lags')<br>    st.write(df[['open', 'close', 'volume', 'returns'] + lagnames].sort_index(ascending=False))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3L_A9EoMWyeeSszJVkpmLg.png\"><figcaption>Displayed DataFrame</figcaption></figure><ul><li>Split data into feature variables and target variable, instantiate the model, train test split the data, then train the data and make predictions</li></ul>\n<pre>#Build, train and test model<br>X = df[['open'] + lagnames]<br>y = df['close']<br><br>#Instantiate the model<br>lr = LinearRegression()<br><br>#Train test split<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)<br><br>#Train on training data<br>lr.fit(X_train, y_train)<br><br>#Predictions <br>y_pred = lr.predict(X_test)</pre>\n<ul><li>Create and display plot comparing the actual and predicted prices</li></ul>\n<pre>#Create plot<br>fig, ax = plt.subplots(figsize=(12, 6))<br>ax.plot(y_test.index, y_test, label='Actual Close Price', color='blue')<br>ax.plot(y_test.index, y_pred, label='Predicted Close Price', color='red')<br>ax.set_title('Actual vs Predicted Close Price of Test Sample (Most Recent 1/5 of Data)')<br>ax.legend()<br>plt.xticks(rotation=45)<br><br>#Display the plot in Streamlit<br>st.pyplot(fig)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9-gjlZaXJcyInWTpO1FHUA.png\"><figcaption>Displayed Plot</figcaption></figure><ul><li>Calculate and display performance metrics for our\u00a0model</li></ul>\n<pre>#Calculate metrics<br>mse = metrics.mean_squared_error(y_test, y_pred)<br>rmse = np.sqrt(mse)<br>mae = metrics.mean_absolute_error(y_test, y_pred)<br>r2 = metrics.r2_score(y_test, y_pred)<br><br>#Display metrics<br>metrics_df = pd.DataFrame({'Metric': ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-squared Score'],'Value': [mse, rmse, mae, r2]})<br>st.table(metrics_df.style.format({'Value': '{:.6f}'}))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IP5KoJE1KrDcP2pRgfxTEA.png\"><figcaption>Display Metrics</figcaption></figure><ul>\n<li>Display the prediction for the upcoming closing\u00a0price</li>\n<li>Show dataframe which includes actual and predicted price, as well as predicted actions</li>\n<li>Calculate and display trading signal\u00a0accuracy</li>\n</ul>\n<pre>#Prediction for the latest data point<br>latest_data = X.iloc[-1].values.reshape(1, -1)<br>latest_prediction = lr.predict(latest_data)[0]<br><br>st.subheader(f\"Prediction for Upcoming Close Price: {latest_prediction}\")<br><br>#Create DataFrame with actual and predicted values, as well as actual and predicted actions<br>results_df = pd.DataFrame({<br>    'Open': X_test['open'],<br>    'Actual Close': y_test,<br>    'Predicted Close': y_pred,<br>})<br>results_df['Actual Price Movement'] = np.where(results_df['Actual Close'] &gt; results_df['Open'], 'Increase', 'Decrease')<br>results_df['Predicted Price Action'] = np.where(results_df['Predicted Close'] &gt; results_df['Open'], 'Buy', 'Sell')<br><br>#Round the values to 2 decimal places<br>results_df = results_df.round(2).sort_index(ascending=False)<br><br>#Style dataframe<br>styled_df = results_df.style.applymap(color_text, subset=['Actual Price Movement', 'Predicted Price Action'])<br><br>st.dataframe(styled_df)<br><br>#Calculate trading signal accuracy<br>correct_predictions = (results_df['Actual Price Movement'] == 'Increase') &amp; (results_df['Predicted Price Action'] == 'Buy') | \\<br>                      (results_df['Actual Price Movement'] == 'Decrease') &amp; (results_df['Predicted Price Action'] == 'Sell')<br>accuracy = correct_predictions.mean()<br><br>#Display accuracy<br>st.write(f\"Trading Signal Accuracy: {accuracy:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SeF85IDwmkG_RjOOgzFVxQ.png\"><figcaption>Display DataFrame with Updated\u00a0Columns</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*tHFMbkJvopr3PoJD.gif\"><figcaption>Linear Regression Page</figcaption></figure><p><strong><em>Logistic Regression Tab</em></strong></p>\n<ul>\n<li>Initialize logistic regression tab with centred\u00a0title</li>\n<li>Calculate and set up both returns and direction (of price) columns, as well as lagged features columns using our function lagit_dir(df, lags)</li>\n<li>Split data into feature variables and target variable, instantiate the model, train test split the data, then train the data and make predictions</li>\n</ul>\n<pre>with tab3:<br>    #Centered title<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Predictions Using Logistic Regression&lt;/h2&gt;\", unsafe_allow_html=True)<br><br>    #Set up returns and direction<br>    df['returns'] = np.log(df.close.pct_change() + 1)<br>    df['direction'] = [1 if i&gt;0 else 0 for i in df.returns]<br><br>    dirnames = lagit_dir(df, 5)<br><br>    df.dropna(inplace=True)<br><br>    X = df[['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5']+dirnames]<br>    y = df['direction']<br><br>    log_reg = LogisticRegression()<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)<br><br>    log_reg.fit(X_train, y_train)<br><br>    y_pred = log_reg.predict(X_test)</pre>\n<ul>\n<li>Display DataFrame with designated columns\u00a0given</li>\n<li>Builds a confusion matrix using actual vs predicted prices, and displays the confusion matrix in Streamlit</li>\n</ul>\n<pre>#Display DataFrame<br>st.subheader('Data with Calculated Lags and Direction')<br>st.dataframe(df[['open', 'close', 'volume', 'returns', 'direction'] + ['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5'] + dirnames].sort_index(ascending=False))<br><br>#Confusion Matrix<br>st.subheader('Confusion Matrix')<br>cm = metrics.confusion_matrix(y_test, y_pred)<br>fig, ax = plt.subplots()<br>sns.heatmap(cm, annot=True, fmt='d')<br>plt.title('Confusion Matrix')<br>plt.ylabel('Actual')<br>plt.xlabel('Predicted')<br>st.pyplot(fig)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*e2TYdPG4vaC6BdWmZi5Whg.png\"><figcaption>Display Confusion Matrix</figcaption></figure><ul><li>Shows classification report and makes prediction as to whether or not the close price will be higher (long) or lower (short) than the open price for a given timeframe</li></ul>\n<pre>#Classification Report<br>st.subheader('Classification Report')<br>report = metrics.classification_report(y_test, y_pred, output_dict=True)<br>st.table(pd.DataFrame(report).transpose())<br><br>#Prediction for latest time<br>latest_data = X.iloc[-1].values.reshape(1, -1)<br>latest_prediction = log_reg.predict(latest_data)[0]<br>latest_probability = log_reg.predict_proba(latest_data)[0][1]<br><br>if latest_prediction == 1:<br>    st.subheader(f\"Prediction for Upcoming Close Price: Long\")<br>else:<br>    st.subheader(f\"Prediction for Upcoming Close Price: Short\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zb8FP8F6BSJh1MPdwF73Ng.png\"><figcaption>Display Classification Report</figcaption></figure><ul>\n<li>Create a DataFrame which contains actual vs predicted price movement, apply color coding using color_text(val)\u00a0, and display the DataFrame</li>\n<li>Calculate and display the trading signal accuracy for our\u00a0model</li>\n</ul>\n<pre>#Create dataframe which shows y_test and y_pred<br>results_df = pd.DataFrame({<br>    'Actual Price Movement': y_test,<br>    'Predicted Action': y_pred<br>})<br>results_df['Open'] = df.loc[results_df.index, 'open']<br>results_df['Close'] = df.loc[results_df.index, 'close']<br>results_df['Price Change %'] = ((results_df['Close'] - results_df['Open']) / results_df['Open'] * 100).round(2)<br><br>column_order = ['Open', 'Close', 'Actual Price Movement', 'Predicted Action', 'Price Change %']<br>results_df = results_df[column_order]<br><br>#Convert binary values to text<br>results_df['Actual Price Movement'] = results_df['Actual Price Movement'].map({1: 'Price Up', 0: 'Price Down'})<br>results_df['Predicted Action'] = results_df['Predicted Action'].map({1: 'Long', 0: 'Short'})<br><br>#Sort the DataFrame by date in descending order<br>results_df = results_df.sort_index(ascending=False)<br><br>#Define color function<br>def color_text(val):<br>    if val in ['Price Up', 'Long']:<br>        return 'color: green'<br>    elif val in ['Price Down', 'Short']:<br>        return 'color: red'<br>    return ''<br><br>#Style dataframe with above function<br>styled_df = results_df.style.applymap(color_text)<br>#Apply color to price change percentage<br>styled_df = styled_df.applymap(lambda v: 'color: green' if v &gt; 0 else 'color: red' if v &lt; 0 else '', subset=['Price Change %'])  <br><br>#Display the styled DataFrame<br>st.dataframe(styled_df)<br><br>#Model Accuracy<br>correct_predictions = ((results_df['Actual Price Movement'] == 'Price Up') &amp; (results_df['Predicted Action'] == 'Long')) | \\<br>                      ((results_df['Actual Price Movement'] == 'Price Down') &amp; (results_df['Predicted Action'] == 'Short'))<br>accuracy = correct_predictions.mean()<br><br>st.write(f\"Trading Signal Accuracy: {accuracy:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Mytxf4wXe4kVMMjj3RBqYw.png\"><figcaption>Display DataFrame with Updated\u00a0Columns</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*wuDJHClRr5Lizdtk.gif\"><figcaption>Logistic Regression Page</figcaption></figure><p><strong><em>Random Forest Regressor Tab</em></strong></p>\n<ul><li>Process identical to our Linear Regression section, other than the model we instantiate is RandomForestRegressor()</li></ul>\n<pre>with tab4:<br>    #Centered title<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Predictions Using Random Forest Regressor&lt;/h2&gt;\", unsafe_allow_html=True)<br><br>    X = df[['open', 'Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5']+dirnames]<br>    y = df['close']<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)<br><br>    rf = RandomForestRegressor(n_estimators=100, random_state=42)<br><br>    rf.fit(X_train, y_train)<br>    y_pred = rf.predict(X_test)<br><br>    #Display DataFrame<br>    st.subheader('Data with Calculated Lags')<br>    st.dataframe(df[['open', 'close', 'volume', 'returns'] + ['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5'] + dirnames].sort_index(ascending=False))<br><br>    #Create and display plot<br>    fig, ax = plt.subplots(figsize=(12, 6))<br>    ax.plot(y_test.index, y_test, label='Actual Close Price', color='blue')<br>    ax.plot(y_test.index, y_pred, label='Predicted Close Price', color='red')<br>    ax.set_title('Actual vs Predicted Close Price of Test Sample (Most Recent 1/5 of Data)')<br>    ax.legend()<br>    plt.xticks(rotation=45)<br>    st.pyplot(fig)<br><br>    #Calculate metrics<br>    mse = metrics.mean_squared_error(y_test, y_pred)<br>    rmse = np.sqrt(mse)<br>    mae = metrics.mean_absolute_error(y_test, y_pred)<br>    r2 = metrics.r2_score(y_test, y_pred)<br><br>    #Display metrics<br>    metrics_df = pd.DataFrame({'Metric': ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-squared Score'],'Value': [mse, rmse, mae, r2]})<br>    st.table(metrics_df.style.format({'Value': '{:.6f}'}))<br><br>    #Prediction for the latest data point<br>    latest_data = X.iloc[-1].values.reshape(1, -1)<br>    latest_prediction = rf.predict(latest_data)[0]<br><br>    st.subheader(f\"Prediction for Upcoming Close Price: {latest_prediction}\")<br><br>    #Create DataFrame with actual and predicted values, as well as actual and predicted actions<br>    results_df = pd.DataFrame({<br>        'Open': X_test['open'],<br>        'Actual Close': y_test,<br>        'Predicted Close': y_pred,<br>    })<br>    results_df['Actual Price Movement'] = np.where(results_df['Actual Close'] &gt; results_df['Open'], 'Increase', 'Decrease')<br>    results_df['Predicted Price Action'] = np.where(results_df['Predicted Close'] &gt; results_df['Open'], 'Buy', 'Sell')<br><br>    #Round the values to 2 decimal places<br>    results_df = results_df.round(2).sort_index(ascending=False)<br><br>    #Style dataframe<br>    styled_df = results_df.style.applymap(color_text, subset=['Actual Price Movement', 'Predicted Price Action'])<br><br>    st.dataframe(styled_df)<br><br>    correct_predictions = (results_df['Actual Price Movement'] == 'Increase') &amp; (results_df['Predicted Price Action'] == 'Buy') | \\<br>                          (results_df['Actual Price Movement'] == 'Decrease') &amp; (results_df['Predicted Price Action'] == 'Sell')<br>    accuracy = correct_predictions.mean()<br><br>    st.write(f\"Trading Signal Accuracy: {accuracy:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*lK0T1TPqRjKoFr3K.gif\"><figcaption>Random Forest Regressor Page</figcaption></figure><p><strong><em>Random Forest Classification Tab</em></strong></p>\n<ul><li>Process identical to our Logistic Regression section, other than the model we instantiate is RandomForestClassifier()</li></ul>\n<pre>with tab5:<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Predictions Using Random Forest Classification&lt;/h2&gt;\", unsafe_allow_html=True)<br><br>    #Set up returns and direction<br>    df['returns'] = np.log(df.close.pct_change() + 1)<br>    df['direction'] = [1 if i&gt;0 else 0 for i in df.returns]<br><br>    dirnames = lagit_dir(df, 5)<br><br>    df.dropna(inplace=True)<br><br>    X = df[['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5']+dirnames]<br>    y = df['direction']<br><br>    rfc = RandomForestClassifier(n_estimators=100, min_samples_split=100, random_state=42)<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)<br><br>    rfc.fit(X_train, y_train)<br><br>    y_pred = rfc.predict(X_test)<br><br>    #Display DataFrame<br>    st.subheader('Data with Calculated Lags and Direction')<br>    st.dataframe(df[['open', 'close', 'volume', 'returns', 'direction'] + ['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5'] + dirnames].sort_index(ascending=False))<br><br>    #Confusion Matrix<br>    st.subheader('Confusion Matrix')<br>    cm = metrics.confusion_matrix(y_test, y_pred)<br>    fig, ax = plt.subplots()<br>    sns.heatmap(cm, annot=True, fmt='d')<br>    plt.title('Confusion Matrix')<br>    plt.ylabel('Actual')<br>    plt.xlabel('Predicted')<br>    st.pyplot(fig)<br><br>    #Classification Report<br>    st.subheader('Classification Report')<br>    report = metrics.classification_report(y_test, y_pred, output_dict=True)<br>    st.table(pd.DataFrame(report).transpose())<br><br>    #Prediction for latest time<br>    latest_data = X.iloc[-1].values.reshape(1, -1)<br>    latest_prediction = rfc.predict(latest_data)[0]<br>    latest_probability = rfc.predict_proba(latest_data)[0][1]<br><br>    if latest_prediction == 1:<br>        st.subheader(f\"Prediction for Upcoming Close Price: Long\")<br>    else:<br>        st.subheader(f\"Prediction for Upcoming Close Price: Short\")<br><br>    #st.write(f'Probability of price going up: {latest_probability:.2f}')<br><br>    #Create dataframe which shows y_test and y_pred<br>    results_df = pd.DataFrame({<br>    'Actual Price Movement': y_test,<br>    'Predicted Action': y_pred<br>    },)<br>    results_df['Open'] = df.loc[results_df.index, 'open']<br>    results_df['Close'] = df.loc[results_df.index, 'close']<br>    results_df['Price Change %'] = ((results_df['Close'] - results_df['Open']) / results_df['Open'] * 100).round(2)<br><br><br>    column_order = ['Open', 'Close', 'Actual Price Movement', 'Predicted Action', 'Price Change %']<br>    results_df = results_df[column_order]<br><br>    #Convert binary values to text<br>    results_df['Actual Price Movement'] = results_df['Actual Price Movement'].map({1: 'Price Up', 0: 'Price Down'})<br>    results_df['Predicted Action'] = results_df['Predicted Action'].map({1: 'Long', 0: 'Short'})<br><br>    #Sort the DataFrame by date in descending order<br>    results_df = results_df.sort_index(ascending=False)<br><br>    #Define color function<br>    def color_text(val):<br>        if val in ['Price Up', 'Long']:<br>            return 'color: green'<br>        elif val in ['Price Down', 'Short']:<br>            return 'color: red'<br>        return ''<br><br>    #Style dataframe with above function<br>    styled_df = results_df.style.applymap(color_text)<br>    #Apply color to price change percentage<br>    styled_df = styled_df.applymap(lambda v: 'color: green' if v &gt; 0 else 'color: red' if v &lt; 0 else '', subset=['Price Change %'])  <br><br>    #Display the styled DataFrame<br>    st.dataframe(styled_df)<br><br>    #Model Accuracy<br>    correct_predictions = ((results_df['Actual Price Movement'] == 'Price Up') &amp; (results_df['Predicted Action'] == 'Long')) | \\<br>                          ((results_df['Actual Price Movement'] == 'Price Down') &amp; (results_df['Predicted Action'] == 'Short'))<br>    accuracy = correct_predictions.mean()<br><br>    st.write(f\"Trading Signal Accuracy: {accuracy:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*LOt80NtBZytm6cDw.gif\"><figcaption>Random Forest Classification Page</figcaption></figure><p><strong>Conclusion</strong></p>\n<p>In building this comprehensive dashboard, we started by pulling cryptocurrency data directly from Binance, ensuring we had the most up-to-date and relevant information for analysis. Using the Binance API, we fetched historical candlestick data for various symbols and intervals, transforming it into a pandas DataFrame. This data was then enriched with key technical indicators such as EMA, RSI, and ADX, providing a solid foundation for our analysis.</p>\n<p>With this rich dataset, we implemented various machine learning algorithms\u200a\u2014\u200aLinear Regression, Logistic Regression, Random Forest Regressor, and Random Forest Classifier\u200a\u2014\u200ato predict future price movements and trends. Each model was meticulously trained and evaluated, with results visualized through interactive plots and tables in our Streamlit dashboard. This seamless integration of real-time data fetching, advanced analytics, and intuitive visualization not only showcases the power of combining financial data with machine learning but also sets the stage for even more sophisticated predictive models and trading strategies in the future. The journey doesn\u2019t end here; it\u2019s an invitation to explore deeper, refine models, and push the boundaries of what\u2019s possible in financial forecasting.</p>\n<p><em>Article may be updated over time with new tools/methods for both the Data Analysis side as well as the Machine Learning\u00a0side.</em></p>\n<p><em>Please note that while our predictions are based on sophisticated algorithms, cryptocurrency markets are highly volatile, and all predictions are purely for educational/entertainment purposes.</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5726dc964423\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/pythons-gurus/building-a-crypto-binance-dashboard-with-machine-learning-predictions-using-python-streamlit-5726dc964423\">Building a Crypto / Binance Dashboard with Machine Learning Predictions using Python &amp; Streamlit</a> was originally published in <a href=\"https://medium.com/pythons-gurus\">Python\u2019s Gurus</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/1*j_oah2QisrHvRGG5vFB4tA.gif\"><figcaption>Homepage for Streamlit App</figcaption></figure><p>In the fast-paced world of cryptocurrency markets, having access to real-time data and predictive insights can make all the difference. Today, I\u2019m excited to share with you a project that combines the power of data analytics with machine learning: a Crypto Technical Analysis Dashboard. This tool, built using Python and Streamlit, taps into Binance\u2019s API to provide traders with a comprehensive view of the crypto market. From interactive price charts and volume analysis to advanced machine learning predictions, this dashboard is designed to be your all-in-one crypto analysis companion. Whether you\u2019re a seasoned trader looking to refine your strategy or a curious developer interested in the intersection of finance and technology, this article will walk you through the creation of a powerful tool that brings together technical indicators, data visualization, and predictive modelling. Join me as we explore how to leverage Linear Regression, Logistic Regression, and Random Forest algorithms to forecast cryptocurrency price movements, all within an intuitive, user-friendly interface.</p>\n<p><em>Please note that while our predictions are based on sophisticated algorithms, cryptocurrency markets are highly volatile, and all predictions are purely for educational/entertainment purposes.</em></p>\n<p><strong>Part 1</strong> will focus on the <strong>Functions</strong> which were built to be used for the Streamlit app.</p>\n<p><strong>Part 2</strong> will focus on the <strong>Technical Analysis Dashboard</strong>, which includes the various technical indicators and analysis.</p>\n<p><strong>Part 3</strong> will focus on the various <strong>Machine Learning Algorithms</strong>, including linear and logistic regression, as well as a couple random forest\u00a0algos.</p>\n<p>Below, I will include all the necessary imports needed for this project and a link to the github repo. Do note, all associated files can be found within the \u2018medium\u2019\u00a0folder.</p>\n<p><a href=\"https://github.com/bamarasingam/binance_dashboard\">GitHub - bamarasingam/binance_dashboard</a></p>\n<pre>#Libraries<br>import streamlit as st<br>import pandas as pd<br>import pandas_ta as ta<br>import numpy as np<br>import matplotlib.pyplot as plt<br>import seaborn as sns<br>import plotly.graph_objs as go<br>from plotly.subplots import make_subplots<br><br>from binance.client import Client<br>from datetime import datetime, timedelta<br><br>from sklearn.linear_model import LinearRegression<br>from sklearn.linear_model import LogisticRegression<br>from sklearn.ensemble import RandomForestRegressor<br>from sklearn.ensemble import RandomForestClassifier<br><br>from sklearn.model_selection import train_test_split<br>from sklearn import metrics</pre>\n<p><strong>Part 1: Functions</strong></p>\n<p>We will explore a series of Python functions I\u2019ve developed to analyze financial data from Binance and generate insightful visualizations. These functions include data loading, emoji-based indicators, chart creation, and lag analysis. Each function plays a crucial role in simplifying and enhancing the process of financial data analysis. I will skip over some functions\u2019 explanation if they are redundant. Here\u2019s a brief overview of each function:</p>\n<ul><li>\n<strong>load_data(symbol, interval, start_date)</strong>:<em> </em>This function initializes a connection to Binance, retrieves historical candlestick data for a specified symbol and interval, and calculates several technical indicators. The data is then organized into a pandas DataFrame.</li></ul>\n<pre>#Function to load in data from Binance<br>def load_data(symbol, interval, start_date):<br>    #Initalize Binance client<br>    client = Client()<br><br>    #Convert start_date to ms timestamp<br>    start_datetime = datetime.combine(start_date, datetime.min.time())<br>    start_ts = int(start_datetime.timestamp() * 1000)<br><br>    #Fetch candlestick data<br>    candles = client.get_historical_klines(symbol, interval, start_ts)<br><br>    #Create df<br>    df = pd.DataFrame(candles, columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', <br>                                       'close_time', 'quote_asset_volume', 'number_of_trades',<br>                                       'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'])<br><br>    #Convert to datetime<br>    df['time'] = pd.to_datetime(df['timestamp'], unit='ms')<br><br>    #Keep OHLCV + time<br>    df = df[['time', 'open', 'high', 'low', 'close', 'volume']]<br><br>    #Convert values to float (other than time)<br>    for col in ['open', 'high', 'low', 'close', 'volume']:<br>        df[col] = df[col].astype(float)<br><br>    #Format time as string value<br>    #df['time'] = df['time'].dt.strftime('%Y-%m-%d')<br>    df['time'] = pd.to_datetime(df['time'])<br>    df.set_index('time', inplace=True)<br><br>    #Calculate indicators<br>    df.ta.ema(length=20, append=True)<br>    df.ta.ema(length=200, append=True)<br>    df.ta.rsi(length=14, append=True)<br>    df.ta.adx(length=14, append=True)<br>    df.ta.atr(length=14, append=True)<br>    df.ta.bbands(length=14, append=True)<br>    df.ta.macd(length=20,append = True)<br><br>    return df</pre>\n<ul><li>\n<strong>get_returns_emoji(ret_val)</strong>: This function returns a green checkmark emoji if the return value is non-negative and a red circle emoji if it is negative (Other emoji based functions are similar in implementation and reasoning).</li></ul>\n<pre>#Function to get emoji based on returns value<br>def get_returns_emoji(ret_val):<br>    return \":white_check_mark:\" if ret_val &gt;= 0 else \":red_circle:\"</pre>\n<ul><li>\n<strong>create_chart(df, symbol, chart_type)</strong>: This function creates a chart (either candlestick or line) with subplots for price and volume, and includes EMA lines for additional context. The resulting plotly figure is well-structured and visually informative.</li></ul>\n<pre>#Function to create chart<br>def create_chart(df, symbol, chart_type):<br>    #Create subplots<br>    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, <br>                        vertical_spacing=0.1, <br>                        subplot_titles=(f'{symbol} {chart_type} Chart', 'Volume'),<br>                        row_heights=[0.7, 0.3])<br><br>    #Add price chart<br>    if chart_type == \"Candlestick\":<br>        fig.add_trace(<br>            go.Candlestick(x=df.index,<br>                           open=df['open'],<br>                           high=df['high'],<br>                           low=df['low'],<br>                           close=df['close'],<br>                           name=\"Price\"),<br>            row=1, col=1<br>        )<br>    else:  #Line chart<br>        fig.add_trace(<br>            go.Scatter(x=df.index,<br>                       y=df['close'],<br>                       mode='lines',<br>                       name=\"Price\"),<br>            row=1, col=1<br>        )<br><br>    #Add EMA lines<br>    fig.add_trace(<br>        go.Scatter(x=df.index, y=df.EMA_20.values, name='EMA20', line=dict(color='blue')),<br>        row=1, col=1<br>    )<br>    fig.add_trace(<br>        go.Scatter(x=df.index, y=df.EMA_200.values, name='EMA200', line=dict(color='red')),<br>        row=1, col=1<br>    )<br><br>    #Add volume<br>    fig.add_trace(<br>        go.Bar(x=df.index, y=df['volume'], name='Volume'),<br>        row=2, col=1<br>    )<br><br>    #Update layout<br>    fig.update_layout(<br>        title_text=f'{symbol} Historical Data',<br>        xaxis_rangeslider_visible=False,<br>        height=800,  #Increase overall height of the figure<br>        showlegend=True<br>    )<br><br>    #Update y-axis<br>    fig.update_yaxes(title_text=\"Price\", row=1, col=1)<br>    fig.update_yaxes(title_text=\"Volume\", row=2, col=1)<br>    #Update x-axis<br>    fig.update_xaxes(<br>        rangeselector=dict(<br>            buttons=list([<br>                dict(count=1, label=\"1m\", step=\"month\", stepmode=\"backward\"),<br>                dict(count=6, label=\"6m\", step=\"month\", stepmode=\"backward\"),<br>                dict(count=1, label=\"YTD\", step=\"year\", stepmode=\"todate\"),<br>                dict(count=1, label=\"1y\", step=\"year\", stepmode=\"backward\"),<br>                dict(step=\"all\")<br>            ])<br>        ),<br>        row=2, col=1  #Add range selector to bottom subplot<br>    )<br><br>    return fig</pre>\n<ul>\n<li>\n<strong>lagit(df, lags)</strong>: This function generates lagged returns columns for a specified number of lags, to aid in time series analysis.</li>\n<li>\n<strong>lagit_dir(df, lags)</strong>: This function generates lagged returns columns for a specified number of lags, aiding in time series analysis. Also includes directional indicators (1 for positive returns, 0 for negative returns) for the lagged\u00a0returns.</li>\n</ul>\n<pre>#Function for lags using returns<br>def lagit(df, lags):<br>    names = []<br>    for i in range(1, lags+1):<br>        df['Lag_' + str(i)] = df['returns'].shift(i)<br>        names.append('Lag_' + str(i))<br>    return names<br><br>#Updated lag function which includes lag direction<br>def lagit_dir(df, lags):<br>    names = []<br>    for i in range(1, lags+1):<br>        df['Lag_'+str(i)] = df['returns'].shift(i)<br>        df['Lag_'+str(i)+'_dir'] = [1 if j&gt;0 else 0 for j in df['Lag_'+str(i)]]<br>        names.append('Lag_'+str(i)+'_dir')<br>    return names</pre>\n<ul><li>\n<strong>color_text(val)</strong>: This function assigns a color to specific words (e.g., 'Increase', 'Buy', 'Decrease', 'Sell') for better visualization, using green for positive actions and red for negative\u00a0ones.</li></ul>\n<pre>#Colors for trigger words<br>def color_text(val):<br>    if val in ['Increase', 'Buy']:<br>        return 'color: green'<br>    elif val in ['Decrease', 'Sell']:<br>        return 'color: red'<br>    return ''</pre>\n<p><strong>Part 2: Technical Analysis Dashboard</strong></p>\n<p>In this section, we\u2019ll create a technical analysis dashboard to analyze cryptocurrency data from Binance. Users can input a cryptocurrency symbol, select data intervals, choose a start date, and pick a chart type. The data is loaded and processed to extract key metrics like closing prices, EMA, RSI, and ADX, etc. These metrics, along with the other features will help provide clear insights into market trends and performance.</p>\n<p>First, we will create our various tabs within the Streamlit dashboard, each designated for its own\u00a0purpose.</p>\n<pre>#Create tabs for dashboard<br>tab1, tab2, tab3, tab4, tab5 = st.tabs([\"Data/Analytics\", \"Lin Regression\", \"Log Regression\", \"RF Regressor\", \"RF Classification\"])</pre>\n<p>Once completed, we will initialize our first tab and center the\u00a0title.</p>\n<pre>with tab1:<br>    #Centered title<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Technical Analysis Dashboard&lt;/h2&gt;\", unsafe_allow_html=True) #Center title</pre>\n<p>Next, we add interactive sidebar components for user input. These include the cryptocurrency ticker symbol, time interval, start date, and chart type. Additionally, an informational note is displayed in the\u00a0sidebar.</p>\n<pre>#Sidebar Components<br>    symbol = st.sidebar.text_input(\"Crypto Symbol (ex. BTCUSDT)\\n\\nMust be a ticker from Binance\", \"BTCUSDT\")<br>    intervals = ('1m', '3m', '5m', '15m', '30m', '1h', '2h', '4h', '6h', '8h', '12h', '1d', '3d', '1w', '1M')<br>    default_index = intervals.index('1d')<br>    interval = st.sidebar.selectbox(\"Interval\", <br>                                    options=intervals,<br>                                    index=default_index)<br><br>    #Date input for start date<br>    default_date = datetime.now().date() - timedelta(days=365)<br>    start_date = st.sidebar.date_input(\"Start Date\", <br>                                    value= default_date,<br>                                    max_value=datetime.now())<br>    chart_type = st.sidebar.radio(\"Chart Type\", (\"Candlestick\", \"Line\"))<br><br>    st.sidebar.info(\"Note: The latest row is within a timeframe which has not completed. The close price is the price at the time at which you pulled the data for this most recent row.\")</pre>\n<p>Now it\u2019s time to load and preprocess our data. The load_data function is called to retrieve and process the data. The DataFrame is reversed to show the most recent data first. Key values such as closing price, EMA, RSI, ADX, and directional movement are extracted from the first row of the reversed DataFrame (to be used later). We will also need to calculate the returns percentages over different time periods, based on the closing prices at those intervals.</p>\n<pre>df = load_data(symbol, interval, start_date)<br>reversed_df = df.iloc[::-1] #Reversed dataframe to be shown in Streamlit<br>row1_val = reversed_df.iloc[0]['close']<br>ema20_val = reversed_df.iloc[0]['EMA_20']<br>ema200_val = reversed_df.iloc[0]['EMA_200']<br>rsi_val = reversed_df.iloc[0]['RSI_14']<br>adx = reversed_df.iloc[0]['ADX_14']<br>dmp = reversed_df.iloc[0]['DMP_14']<br>dmn = reversed_df.iloc[0]['DMN_14']<br><br>row20_val = reversed_df.iloc[20]['close'] if len(reversed_df) &gt; 20 else row1_val<br>row60_val = reversed_df.iloc[60]['close'] if len(reversed_df) &gt; 60 else row1_val<br>row120_val = reversed_df.iloc[120]['close'] if len(reversed_df) &gt; 120 else row1_val<br>row240_val = reversed_df.iloc[240]['close'] if len(reversed_df) &gt; 240 else row1_val<br><br>day20_ret_percent = (row1_val - row20_val)/row20_val * 100<br>day60_ret_percent = (row1_val - row60_val)/row60_val * 100<br>day120_ret_percent = (row1_val - row120_val)/row120_val * 100<br>day240_ret_percent = (row1_val - row240_val)/row240_val * 100</pre>\n<p>Once calculated, we can now display the above data. This below code uses Streamlit columns to display the data in a structured format. Each column contains specific metrics: returns, momentum indicators, and trend strength indicators, along with corresponding emojis for visual cues (remember we built these functions earlier).</p>\n<pre>#Displays (column wide)<br>col1, col2, col3 = st.columns(3)<br>with col1:<br>    st.subheader(\"Returns\")<br>    st.markdown(f\"- 1 MONTH : {round(day20_ret_percent,2)}% {get_returns_emoji(round(day20_ret_percent,2))}\")<br>    st.markdown(f\"- 3 MONTHS : {round(day60_ret_percent,2)}% {get_returns_emoji(round(day60_ret_percent,2))}\")<br>    st.markdown(f\"- 6 MONTHS : {round(day120_ret_percent,2)}% {get_returns_emoji(round(day120_ret_percent,2))}\")<br>    st.markdown(f\"- 12 MONTHS : {round(day240_ret_percent,2)}% {get_returns_emoji(round(day240_ret_percent,2))}\")<br>with col2:<br>    st.subheader(\"Momentum\")<br>    st.markdown(f\"- LTP : {round(row1_val,2)}\")<br>    st.markdown(f\"- EMA20 : {round(ema20_val,2)} {get_ema_emoji(round(row1_val,2),round(ema20_val,2))}\") <br>    st.markdown(f\"- EMA200 : {round(ema200_val,2)} {get_ema_emoji(round(row1_val,2),round(ema200_val,2))}\") <br>    st.markdown(f\"- RSI : {round(rsi_val,2)} {get_rsi_emoji(round(rsi_val,2))}\") <br>with col3:<br>    st.subheader(\"Trend Strength\")<br>    st.markdown(f\"- ADX : {round(adx,2)} {get_adx_emoji(round(adx,2))}\") <br>    st.markdown(f\"- DMP : {round(dmp,2)} \") <br>    st.markdown(f\"- DMN : {round(dmn,2)} \")</pre>\n<p>Finally, we will call the create_chart function to create and display our price chart. The chart, along with the reversed DataFrame, is shown in the Streamlit app, allowing for interactive data visualization.</p>\n<pre>#Display chart <br>st.plotly_chart(create_chart(df, symbol, chart_type), use_container_width=True)<br>#Display dataframe<br>st.write(reversed_df)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/proxy/1*j_oah2QisrHvRGG5vFB4tA.gif\"><figcaption>Technical Analysis Dashboard</figcaption></figure><p><strong>Part 3: Machine Learning Algorithms</strong></p>\n<p>In the final section, we will explore the implementation of various machine learning algorithms to predict cryptocurrency prices and trends using historical data. Each tab in our dashboard is dedicated to a specific algorithm: Linear Regression, Logistic Regression, Random Forest Regressor, and Random Forest Classifier. We will go through the process of setting up each model, training and testing it, and evaluating its performance. Due to the length of this process, I will go through the process in bullet points and skip over redundant parts. Let\u2019s dive into the details of each part of the\u00a0code.</p>\n<p><strong><em>Linear Regression Tab</em></strong></p>\n<ul>\n<li>Initialize our linear regression tab with centred\u00a0title</li>\n<li>Calculate logarithmic returns for our returns column, then call our lagit(df, lags) to create our lagged returns columns, and display the DataFrame within Streamlit</li>\n</ul>\n<pre>#Linear Regression Tab<br>with tab2:<br>    #Centered title<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Predictions Using Linear Regression&lt;/h2&gt;\", unsafe_allow_html=True)<br><br>    #Create returns column<br>    df['returns'] = np.log(df.close.pct_change() + 1)<br><br>    #Call function with associated amount of lags<br>    lagnames = lagit(df, 5)<br>    df.dropna(inplace=True)<br><br>    #Display the DataFrame with calculated lags<br>    st.subheader('Data with Calculated Lags')<br>    st.write(df[['open', 'close', 'volume', 'returns'] + lagnames].sort_index(ascending=False))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*3L_A9EoMWyeeSszJVkpmLg.png\"><figcaption>Displayed DataFrame</figcaption></figure><ul><li>Split data into feature variables and target variable, instantiate the model, train test split the data, then train the data and make predictions</li></ul>\n<pre>#Build, train and test model<br>X = df[['open'] + lagnames]<br>y = df['close']<br><br>#Instantiate the model<br>lr = LinearRegression()<br><br>#Train test split<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)<br><br>#Train on training data<br>lr.fit(X_train, y_train)<br><br>#Predictions <br>y_pred = lr.predict(X_test)</pre>\n<ul><li>Create and display plot comparing the actual and predicted prices</li></ul>\n<pre>#Create plot<br>fig, ax = plt.subplots(figsize=(12, 6))<br>ax.plot(y_test.index, y_test, label='Actual Close Price', color='blue')<br>ax.plot(y_test.index, y_pred, label='Predicted Close Price', color='red')<br>ax.set_title('Actual vs Predicted Close Price of Test Sample (Most Recent 1/5 of Data)')<br>ax.legend()<br>plt.xticks(rotation=45)<br><br>#Display the plot in Streamlit<br>st.pyplot(fig)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*9-gjlZaXJcyInWTpO1FHUA.png\"><figcaption>Displayed Plot</figcaption></figure><ul><li>Calculate and display performance metrics for our\u00a0model</li></ul>\n<pre>#Calculate metrics<br>mse = metrics.mean_squared_error(y_test, y_pred)<br>rmse = np.sqrt(mse)<br>mae = metrics.mean_absolute_error(y_test, y_pred)<br>r2 = metrics.r2_score(y_test, y_pred)<br><br>#Display metrics<br>metrics_df = pd.DataFrame({'Metric': ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-squared Score'],'Value': [mse, rmse, mae, r2]})<br>st.table(metrics_df.style.format({'Value': '{:.6f}'}))</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*IP5KoJE1KrDcP2pRgfxTEA.png\"><figcaption>Display Metrics</figcaption></figure><ul>\n<li>Display the prediction for the upcoming closing\u00a0price</li>\n<li>Show dataframe which includes actual and predicted price, as well as predicted actions</li>\n<li>Calculate and display trading signal\u00a0accuracy</li>\n</ul>\n<pre>#Prediction for the latest data point<br>latest_data = X.iloc[-1].values.reshape(1, -1)<br>latest_prediction = lr.predict(latest_data)[0]<br><br>st.subheader(f\"Prediction for Upcoming Close Price: {latest_prediction}\")<br><br>#Create DataFrame with actual and predicted values, as well as actual and predicted actions<br>results_df = pd.DataFrame({<br>    'Open': X_test['open'],<br>    'Actual Close': y_test,<br>    'Predicted Close': y_pred,<br>})<br>results_df['Actual Price Movement'] = np.where(results_df['Actual Close'] &gt; results_df['Open'], 'Increase', 'Decrease')<br>results_df['Predicted Price Action'] = np.where(results_df['Predicted Close'] &gt; results_df['Open'], 'Buy', 'Sell')<br><br>#Round the values to 2 decimal places<br>results_df = results_df.round(2).sort_index(ascending=False)<br><br>#Style dataframe<br>styled_df = results_df.style.applymap(color_text, subset=['Actual Price Movement', 'Predicted Price Action'])<br><br>st.dataframe(styled_df)<br><br>#Calculate trading signal accuracy<br>correct_predictions = (results_df['Actual Price Movement'] == 'Increase') &amp; (results_df['Predicted Price Action'] == 'Buy') | \\<br>                      (results_df['Actual Price Movement'] == 'Decrease') &amp; (results_df['Predicted Price Action'] == 'Sell')<br>accuracy = correct_predictions.mean()<br><br>#Display accuracy<br>st.write(f\"Trading Signal Accuracy: {accuracy:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*SeF85IDwmkG_RjOOgzFVxQ.png\"><figcaption>Display DataFrame with Updated\u00a0Columns</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*tHFMbkJvopr3PoJD.gif\"><figcaption>Linear Regression Page</figcaption></figure><p><strong><em>Logistic Regression Tab</em></strong></p>\n<ul>\n<li>Initialize logistic regression tab with centred\u00a0title</li>\n<li>Calculate and set up both returns and direction (of price) columns, as well as lagged features columns using our function lagit_dir(df, lags)</li>\n<li>Split data into feature variables and target variable, instantiate the model, train test split the data, then train the data and make predictions</li>\n</ul>\n<pre>with tab3:<br>    #Centered title<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Predictions Using Logistic Regression&lt;/h2&gt;\", unsafe_allow_html=True)<br><br>    #Set up returns and direction<br>    df['returns'] = np.log(df.close.pct_change() + 1)<br>    df['direction'] = [1 if i&gt;0 else 0 for i in df.returns]<br><br>    dirnames = lagit_dir(df, 5)<br><br>    df.dropna(inplace=True)<br><br>    X = df[['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5']+dirnames]<br>    y = df['direction']<br><br>    log_reg = LogisticRegression()<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)<br><br>    log_reg.fit(X_train, y_train)<br><br>    y_pred = log_reg.predict(X_test)</pre>\n<ul>\n<li>Display DataFrame with designated columns\u00a0given</li>\n<li>Builds a confusion matrix using actual vs predicted prices, and displays the confusion matrix in Streamlit</li>\n</ul>\n<pre>#Display DataFrame<br>st.subheader('Data with Calculated Lags and Direction')<br>st.dataframe(df[['open', 'close', 'volume', 'returns', 'direction'] + ['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5'] + dirnames].sort_index(ascending=False))<br><br>#Confusion Matrix<br>st.subheader('Confusion Matrix')<br>cm = metrics.confusion_matrix(y_test, y_pred)<br>fig, ax = plt.subplots()<br>sns.heatmap(cm, annot=True, fmt='d')<br>plt.title('Confusion Matrix')<br>plt.ylabel('Actual')<br>plt.xlabel('Predicted')<br>st.pyplot(fig)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*e2TYdPG4vaC6BdWmZi5Whg.png\"><figcaption>Display Confusion Matrix</figcaption></figure><ul><li>Shows classification report and makes prediction as to whether or not the close price will be higher (long) or lower (short) than the open price for a given timeframe</li></ul>\n<pre>#Classification Report<br>st.subheader('Classification Report')<br>report = metrics.classification_report(y_test, y_pred, output_dict=True)<br>st.table(pd.DataFrame(report).transpose())<br><br>#Prediction for latest time<br>latest_data = X.iloc[-1].values.reshape(1, -1)<br>latest_prediction = log_reg.predict(latest_data)[0]<br>latest_probability = log_reg.predict_proba(latest_data)[0][1]<br><br>if latest_prediction == 1:<br>    st.subheader(f\"Prediction for Upcoming Close Price: Long\")<br>else:<br>    st.subheader(f\"Prediction for Upcoming Close Price: Short\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*zb8FP8F6BSJh1MPdwF73Ng.png\"><figcaption>Display Classification Report</figcaption></figure><ul>\n<li>Create a DataFrame which contains actual vs predicted price movement, apply color coding using color_text(val)\u00a0, and display the DataFrame</li>\n<li>Calculate and display the trading signal accuracy for our\u00a0model</li>\n</ul>\n<pre>#Create dataframe which shows y_test and y_pred<br>results_df = pd.DataFrame({<br>    'Actual Price Movement': y_test,<br>    'Predicted Action': y_pred<br>})<br>results_df['Open'] = df.loc[results_df.index, 'open']<br>results_df['Close'] = df.loc[results_df.index, 'close']<br>results_df['Price Change %'] = ((results_df['Close'] - results_df['Open']) / results_df['Open'] * 100).round(2)<br><br>column_order = ['Open', 'Close', 'Actual Price Movement', 'Predicted Action', 'Price Change %']<br>results_df = results_df[column_order]<br><br>#Convert binary values to text<br>results_df['Actual Price Movement'] = results_df['Actual Price Movement'].map({1: 'Price Up', 0: 'Price Down'})<br>results_df['Predicted Action'] = results_df['Predicted Action'].map({1: 'Long', 0: 'Short'})<br><br>#Sort the DataFrame by date in descending order<br>results_df = results_df.sort_index(ascending=False)<br><br>#Define color function<br>def color_text(val):<br>    if val in ['Price Up', 'Long']:<br>        return 'color: green'<br>    elif val in ['Price Down', 'Short']:<br>        return 'color: red'<br>    return ''<br><br>#Style dataframe with above function<br>styled_df = results_df.style.applymap(color_text)<br>#Apply color to price change percentage<br>styled_df = styled_df.applymap(lambda v: 'color: green' if v &gt; 0 else 'color: red' if v &lt; 0 else '', subset=['Price Change %'])  <br><br>#Display the styled DataFrame<br>st.dataframe(styled_df)<br><br>#Model Accuracy<br>correct_predictions = ((results_df['Actual Price Movement'] == 'Price Up') &amp; (results_df['Predicted Action'] == 'Long')) | \\<br>                      ((results_df['Actual Price Movement'] == 'Price Down') &amp; (results_df['Predicted Action'] == 'Short'))<br>accuracy = correct_predictions.mean()<br><br>st.write(f\"Trading Signal Accuracy: {accuracy:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*Mytxf4wXe4kVMMjj3RBqYw.png\"><figcaption>Display DataFrame with Updated\u00a0Columns</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*wuDJHClRr5Lizdtk.gif\"><figcaption>Logistic Regression Page</figcaption></figure><p><strong><em>Random Forest Regressor Tab</em></strong></p>\n<ul><li>Process identical to our Linear Regression section, other than the model we instantiate is RandomForestRegressor()</li></ul>\n<pre>with tab4:<br>    #Centered title<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Predictions Using Random Forest Regressor&lt;/h2&gt;\", unsafe_allow_html=True)<br><br>    X = df[['open', 'Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5']+dirnames]<br>    y = df['close']<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)<br><br>    rf = RandomForestRegressor(n_estimators=100, random_state=42)<br><br>    rf.fit(X_train, y_train)<br>    y_pred = rf.predict(X_test)<br><br>    #Display DataFrame<br>    st.subheader('Data with Calculated Lags')<br>    st.dataframe(df[['open', 'close', 'volume', 'returns'] + ['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5'] + dirnames].sort_index(ascending=False))<br><br>    #Create and display plot<br>    fig, ax = plt.subplots(figsize=(12, 6))<br>    ax.plot(y_test.index, y_test, label='Actual Close Price', color='blue')<br>    ax.plot(y_test.index, y_pred, label='Predicted Close Price', color='red')<br>    ax.set_title('Actual vs Predicted Close Price of Test Sample (Most Recent 1/5 of Data)')<br>    ax.legend()<br>    plt.xticks(rotation=45)<br>    st.pyplot(fig)<br><br>    #Calculate metrics<br>    mse = metrics.mean_squared_error(y_test, y_pred)<br>    rmse = np.sqrt(mse)<br>    mae = metrics.mean_absolute_error(y_test, y_pred)<br>    r2 = metrics.r2_score(y_test, y_pred)<br><br>    #Display metrics<br>    metrics_df = pd.DataFrame({'Metric': ['Mean Squared Error', 'Root Mean Squared Error', 'Mean Absolute Error', 'R-squared Score'],'Value': [mse, rmse, mae, r2]})<br>    st.table(metrics_df.style.format({'Value': '{:.6f}'}))<br><br>    #Prediction for the latest data point<br>    latest_data = X.iloc[-1].values.reshape(1, -1)<br>    latest_prediction = rf.predict(latest_data)[0]<br><br>    st.subheader(f\"Prediction for Upcoming Close Price: {latest_prediction}\")<br><br>    #Create DataFrame with actual and predicted values, as well as actual and predicted actions<br>    results_df = pd.DataFrame({<br>        'Open': X_test['open'],<br>        'Actual Close': y_test,<br>        'Predicted Close': y_pred,<br>    })<br>    results_df['Actual Price Movement'] = np.where(results_df['Actual Close'] &gt; results_df['Open'], 'Increase', 'Decrease')<br>    results_df['Predicted Price Action'] = np.where(results_df['Predicted Close'] &gt; results_df['Open'], 'Buy', 'Sell')<br><br>    #Round the values to 2 decimal places<br>    results_df = results_df.round(2).sort_index(ascending=False)<br><br>    #Style dataframe<br>    styled_df = results_df.style.applymap(color_text, subset=['Actual Price Movement', 'Predicted Price Action'])<br><br>    st.dataframe(styled_df)<br><br>    correct_predictions = (results_df['Actual Price Movement'] == 'Increase') &amp; (results_df['Predicted Price Action'] == 'Buy') | \\<br>                          (results_df['Actual Price Movement'] == 'Decrease') &amp; (results_df['Predicted Price Action'] == 'Sell')<br>    accuracy = correct_predictions.mean()<br><br>    st.write(f\"Trading Signal Accuracy: {accuracy:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*lK0T1TPqRjKoFr3K.gif\"><figcaption>Random Forest Regressor Page</figcaption></figure><p><strong><em>Random Forest Classification Tab</em></strong></p>\n<ul><li>Process identical to our Logistic Regression section, other than the model we instantiate is RandomForestClassifier()</li></ul>\n<pre>with tab5:<br>    st.markdown(\"&lt;h2 style='text-align: center;'&gt;Predictions Using Random Forest Classification&lt;/h2&gt;\", unsafe_allow_html=True)<br><br>    #Set up returns and direction<br>    df['returns'] = np.log(df.close.pct_change() + 1)<br>    df['direction'] = [1 if i&gt;0 else 0 for i in df.returns]<br><br>    dirnames = lagit_dir(df, 5)<br><br>    df.dropna(inplace=True)<br><br>    X = df[['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5']+dirnames]<br>    y = df['direction']<br><br>    rfc = RandomForestClassifier(n_estimators=100, min_samples_split=100, random_state=42)<br><br>    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=42)<br><br>    rfc.fit(X_train, y_train)<br><br>    y_pred = rfc.predict(X_test)<br><br>    #Display DataFrame<br>    st.subheader('Data with Calculated Lags and Direction')<br>    st.dataframe(df[['open', 'close', 'volume', 'returns', 'direction'] + ['Lag_1', 'Lag_2', 'Lag_3', 'Lag_4', 'Lag_5'] + dirnames].sort_index(ascending=False))<br><br>    #Confusion Matrix<br>    st.subheader('Confusion Matrix')<br>    cm = metrics.confusion_matrix(y_test, y_pred)<br>    fig, ax = plt.subplots()<br>    sns.heatmap(cm, annot=True, fmt='d')<br>    plt.title('Confusion Matrix')<br>    plt.ylabel('Actual')<br>    plt.xlabel('Predicted')<br>    st.pyplot(fig)<br><br>    #Classification Report<br>    st.subheader('Classification Report')<br>    report = metrics.classification_report(y_test, y_pred, output_dict=True)<br>    st.table(pd.DataFrame(report).transpose())<br><br>    #Prediction for latest time<br>    latest_data = X.iloc[-1].values.reshape(1, -1)<br>    latest_prediction = rfc.predict(latest_data)[0]<br>    latest_probability = rfc.predict_proba(latest_data)[0][1]<br><br>    if latest_prediction == 1:<br>        st.subheader(f\"Prediction for Upcoming Close Price: Long\")<br>    else:<br>        st.subheader(f\"Prediction for Upcoming Close Price: Short\")<br><br>    #st.write(f'Probability of price going up: {latest_probability:.2f}')<br><br>    #Create dataframe which shows y_test and y_pred<br>    results_df = pd.DataFrame({<br>    'Actual Price Movement': y_test,<br>    'Predicted Action': y_pred<br>    },)<br>    results_df['Open'] = df.loc[results_df.index, 'open']<br>    results_df['Close'] = df.loc[results_df.index, 'close']<br>    results_df['Price Change %'] = ((results_df['Close'] - results_df['Open']) / results_df['Open'] * 100).round(2)<br><br><br>    column_order = ['Open', 'Close', 'Actual Price Movement', 'Predicted Action', 'Price Change %']<br>    results_df = results_df[column_order]<br><br>    #Convert binary values to text<br>    results_df['Actual Price Movement'] = results_df['Actual Price Movement'].map({1: 'Price Up', 0: 'Price Down'})<br>    results_df['Predicted Action'] = results_df['Predicted Action'].map({1: 'Long', 0: 'Short'})<br><br>    #Sort the DataFrame by date in descending order<br>    results_df = results_df.sort_index(ascending=False)<br><br>    #Define color function<br>    def color_text(val):<br>        if val in ['Price Up', 'Long']:<br>            return 'color: green'<br>        elif val in ['Price Down', 'Short']:<br>            return 'color: red'<br>        return ''<br><br>    #Style dataframe with above function<br>    styled_df = results_df.style.applymap(color_text)<br>    #Apply color to price change percentage<br>    styled_df = styled_df.applymap(lambda v: 'color: green' if v &gt; 0 else 'color: red' if v &lt; 0 else '', subset=['Price Change %'])  <br><br>    #Display the styled DataFrame<br>    st.dataframe(styled_df)<br><br>    #Model Accuracy<br>    correct_predictions = ((results_df['Actual Price Movement'] == 'Price Up') &amp; (results_df['Predicted Action'] == 'Long')) | \\<br>                          ((results_df['Actual Price Movement'] == 'Price Down') &amp; (results_df['Predicted Action'] == 'Short'))<br>    accuracy = correct_predictions.mean()<br><br>    st.write(f\"Trading Signal Accuracy: {accuracy:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/600/0*LOt80NtBZytm6cDw.gif\"><figcaption>Random Forest Classification Page</figcaption></figure><p><strong>Conclusion</strong></p>\n<p>In building this comprehensive dashboard, we started by pulling cryptocurrency data directly from Binance, ensuring we had the most up-to-date and relevant information for analysis. Using the Binance API, we fetched historical candlestick data for various symbols and intervals, transforming it into a pandas DataFrame. This data was then enriched with key technical indicators such as EMA, RSI, and ADX, providing a solid foundation for our analysis.</p>\n<p>With this rich dataset, we implemented various machine learning algorithms\u200a\u2014\u200aLinear Regression, Logistic Regression, Random Forest Regressor, and Random Forest Classifier\u200a\u2014\u200ato predict future price movements and trends. Each model was meticulously trained and evaluated, with results visualized through interactive plots and tables in our Streamlit dashboard. This seamless integration of real-time data fetching, advanced analytics, and intuitive visualization not only showcases the power of combining financial data with machine learning but also sets the stage for even more sophisticated predictive models and trading strategies in the future. The journey doesn\u2019t end here; it\u2019s an invitation to explore deeper, refine models, and push the boundaries of what\u2019s possible in financial forecasting.</p>\n<p><em>Article may be updated over time with new tools/methods for both the Data Analysis side as well as the Machine Learning\u00a0side.</em></p>\n<p><em>Please note that while our predictions are based on sophisticated algorithms, cryptocurrency markets are highly volatile, and all predictions are purely for educational/entertainment purposes.</em></p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5726dc964423\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/pythons-gurus/building-a-crypto-binance-dashboard-with-machine-learning-predictions-using-python-streamlit-5726dc964423\">Building a Crypto / Binance Dashboard with Machine Learning Predictions using Python &amp; Streamlit</a> was originally published in <a href=\"https://medium.com/pythons-gurus\">Python\u2019s Gurus</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["binance","machine-learning","python","cryptocurrency","streamlit"]},{"title":"Exploring Optimal Portfolio Construction with Riskfolio-Lib and Cryptocurrencies","pubDate":"2024-06-04 22:56:22","link":"https://medium.com/pythons-gurus/exploring-optimal-portfolio-construction-with-riskfolio-lib-and-cryptocurrencies-6d2321e053a6?source=rss-ce7453a5c978------2","guid":"https://medium.com/p/6d2321e053a6","author":"Brandon Amarasingam","thumbnail":"","description":"\n<p><em>Disclaimer: This article is for entertainment/educational purposes only and does not constitute financial advice. Investing in cryptocurrencies and other assets involves risk. Please do your own research and consult a licensed financial advisor before making any investment decisions. The author is not responsible for any financial losses.</em></p>\n<p>In the fast-paced world of finance, constructing an optimal investment portfolio is a key challenge, especially with cryptocurrencies. Traditional methods often struggle to keep up with the volatility and complexity of modern markets. Riskfolio-Lib, an open-source library for portfolio optimization and risk management, offers a robust solution by providing advanced quantitative tools to build and analyze portfolios with precision.</p>\n<p>This article explores how to leverage Riskfolio-Lib to evaluate and optimize a portfolio that includes cryptocurrencies. Due note, the end of this article explores drawdowns within a portfolio, and takes a deeper dive into the calculation. By balancing risk and return and enhancing diversification, Riskfolio-Lib helps shed a different light in the eyes of analysis for portfolio construction.</p>\n<h3><strong>Riskfolio-Lib</strong></h3>\n<p><a href=\"https://riskfolio-lib.readthedocs.io/en/latest/?source=post_page-----d4e4d503541c--------------------------------#\">Riskfolio-Lib - Riskfolio-Lib 6.1.1 documentation</a></p>\n<p>Riskfolio is an open-source Python library designed for financial portfolio optimization, risk management, and analysis, facilitating advanced investment strategies and portfolio construction techniques.</p>\n<p>Although this portfolio has many features, options and capabilities we will be taking a look at a few. If you would like to take a look at my repo, I\u2019ll leave a link\u00a0below:</p>\n<p><a href=\"https://github.com/bamarasingam/risfolio_exploration/tree/main/medium\">https://github.com/bamarasingam/risfolio_exploration/tree/main/medium</a></p>\n<h3><strong>Getting Started</strong></h3>\n<p>To start, you will need to install the following (if you don\u2019t have already):</p>\n<pre>pip install Riskfolio-Lib<br>pip install yfinnance<br>pip install pandas</pre>\n<p>Once installed, we can import our libraries, and read in the necessary data. I decided to go with 5 cryptocurrencies which have been around for some years: BTC, ETH, XRP, DOGE. LINK. I pulled data from the start of the 3rd Bitcoin halving (May 5 2020) until now, taking only the adjusted closing prices during that timespan for each\u00a0asset.</p>\n<pre>import pandas as pd<br>import riskfolio as rp<br>import yfinance as yf<br><br>#Define the ticker symbols for the cryptocurrencies<br>tickers = {<br>    'BTC': 'BTC-USD',<br>    'ETH': 'ETH-USD',<br>    'XRP': 'XRP-USD',<br>    'DOGE': 'DOGE-USD',<br>    'LINK': 'LINK-USD'<br>}<br><br>#Define the start date<br>start_date = '2020-05-11'<br>end_date = pd.Timestamp.today().strftime('%Y-%m-%d')<br><br>#Create a dictionary to store the data<br>data = {}<br><br>#Fetch the data for each ticker<br>for name, ticker in tickers.items():<br>    df = yf.download(ticker, start=start_date, end=end_date)<br>    data[name] = df['Adj Close']<br><br>#Combine the data into a single DataFrame<br>df = pd.DataFrame(data)<br><br>df</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/456/1*fVVeIDyjyi9ENcDJ0Wh2Pg.png\"><figcaption>Output for\u00a0df</figcaption></figure><h3>Building Portfolio</h3>\n<p>Once the data is collected, we can now start to build the portfolio object. First, we calculate the daily returns for each asset by computing the percentage change in asset prices (returns), and then calculate the cumulative returns by compounding these daily returns (cumulative_returns). This is essential for understanding the total growth of the investment over\u00a0time.</p>\n<pre>#Overall returns for each asset<br>returns = df.pct_change().dropna()<br><br>#Calculate the cumulative returns<br>cumulative_returns = (1 + returns).cumprod()</pre>\n<p>Now, we build a portfolio object in Riskfolio-Lib using our returns dataframe. Various other parameters can be tweaked and found on the Riskfolio site linked\u00a0above.</p>\n<pre>#Building portfolio object<br>port = rp.Portfolio(returns, nea = 4) #Number of Effective Assets (NEA)<br><br>port.assets_stats(<br>    method_mu = 'hist', #Selected method to estimate expected returns based on historical data<br>    method_cov = 'hist', #Selected method to estimate covariance matrix based on historical data<br>    d = 0.94)<br><br>port.benchindex = returns_bench</pre>\n<p>Now we can estimate the optimal portfolio weights using Riskfolio-Lib by applying the Classic model and targeting the highest Sharpe ratio, with Conditional Value at Risk (CVaR) as the risk measure, based on historical data.</p>\n<pre>#Estimate optimal portfolio for mean/cvar ratio<br>w = port.optimization(<br>    model = 'Classic', #Either classic, BL(Black Litterman), FM(Factor model), BLFM(Black Litterman w/Factor models)<br>    rm = 'CVaR', #Risk measure<br>    obj = 'Sharpe', #Objective function, could be MinRisk, MaxRet, Utility or Sharpe<br>    hist = True, #Use historical scenarios for risk measures<br>    rf = 0, #Risk free rate<br>    l = 0) #Risk aversion factor, only useful when obj is 'utility'<br>w #Optimal weights for our portfolio</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/119/1*LM6zEgR1P1pfvHx6JA3rbw.png\"><figcaption>Suggested weights for portfolio</figcaption></figure><p>Hooray we\u2019ve got our suggested weights for our portfolio! But what does any of this stuff even mean? How did we get to this conclusion? Riskfolio has a myriad of tools to which you can use and below we will explore a\u00a0few.</p>\n<h3>Efficient Frontier</h3>\n<p>The efficient frontier is a concept in modern portfolio theory that represents a set of optimal portfolios offering the highest expected return for a given level of risk. These portfolios are graphically depicted on a curve, illustrating the trade-off between risk and return. Portfolios on the efficient frontier are considered optimal because they maximize returns while minimizing risk compared to other portfolios with the same risk\u00a0level.</p>\n<pre>#Calculate efficient frontier<br>points = 50<br><br>frontier = port.efficient_frontier(model = 'Classic',<br>                                   rm = 'CVaR',<br>                                   rf = 0,<br>                                   hist = True)<br><br>display(frontier.T.head())<br><br>#Plot efficient frontier<br>ax = rp.plot_frontier(w_frontier = frontier, #Frontier calculated from lines above (weights from efficient frontier)<br>                      mu = port.mu, #Expected returns<br>                      cov = port.cov, #Covariance matrix<br>                      returns = port.returns, #Returns of assets<br>                      rm = 'CVaR', #Risk measure<br>                      rf = 0, #Risk free rate<br>                      alpha = 0.05, #Significance level<br>                      w = w, #Weights of portfolio<br>                      label = 'Max Risk Adjusted Return Portfolio',<br>                      ax = None<br>                      )<br><br>#Plot efficient frontier composition<br>ax = rp.plot_frontier_area(w_frontier = frontier) #Frontier is weights of portfolios in efficient frontier</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/336/1*BnAVhLAsx1p5O9jfB9E9jw.png\"><figcaption>Efficient Frontier\u00a0weights</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/943/1*o02qEioRetwn-K-0FZh6zw.png\"><figcaption>Efficient Frontier\u200a\u2014\u200aDue to the nature of volatility within digital assets, these numbers are a lot higher than a traditional stock portfolio</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/989/1*0_q7MkXffIlyGFrc4AsSEA.png\"><figcaption>Efficient Frontier Composition</figcaption></figure><p>The first image depicts the efficient frontier for a portfolio, highlighting the trade-off between expected return and risk (measured by Conditional Value at Risk, CVaR, at a 5% significance level). The curve shows the set of optimal portfolios that provide the highest expected return for a given level of risk. The red star marks the portfolio with the maximum risk-adjusted return, indicating the best trade-off between risk and return. The color gradient along the curve represents the risk-adjusted return ratio, with warmer colours indicating higher ratios, suggesting that the optimal portfolio has a high risk-adjusted return.</p>\n<p>The second image illustrates the asset allocation across the efficient frontier\u2019s assets structure. This stacked area chart shows how the composition of different assets (BTC, ETH, XRP, DOGE, LINK) changes along the efficient frontier. It indicates that as the portfolio moves along the efficient frontier (from left to right, increasing risk), the allocation to each asset adjusts to maintain optimal risk-return trade-offs. BTC and ETH have significant portions across most of the frontier, while DOGE and LINK have smaller allocations. This visualization helps to understand how diversification changes with varying risk levels, highlighting the importance of different assets in achieving an optimal portfolio.</p>\n<h3>Riskfolio-Lib Report</h3>\n<p>The Riskfolio-Lib Report is a great feature which allows us to output various useful information to analyze risk and profitability of investment portfolios. This includes but not limited to: Historical Compounded Cumulative Returns, Portfolio Composition, Risk Contribution per Asset, Portfolio Returns Histogram, and Historical Uncompounded Drawdown.</p>\n<pre>ax = rp.jupyter_report(returns, #returns dataframe<br>                       w = w, #Portfolio weights<br>                       rm='MV', #Risk measure used <br>                       rf=0, #Risk free rate<br>                       alpha=0.05, #Signifigance level<br>                       t_factor=365, #Factor used to annualize expected return &amp; expected risks <br>                       days_per_year=365) #Number of trading days in year</pre>\n<p>When executed, you get a report with a myriad of information. The one I want to focus on is <strong>Historical Uncompounded Drawdown.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/948/1*kxaERus5Zi0JsQzhIW1e5g.png\"></figure><p>It\u2019s impossible for a drawdown to exceed 100%, yet our Maximum Drawdown says otherwise. I spent a decent amount of time trying to figure out why, referring back to the docs and checking my code. In order to better understand, let\u2019s take a look at the\u00a0formula.</p>\n<a href=\"https://medium.com/media/209db8c1f9ae9844908b849b7c50d040/href\">https://medium.com/media/209db8c1f9ae9844908b849b7c50d040/href</a><p>This formula essentially:</p>\n<a href=\"https://medium.com/media/26763ce91c37a7cf490a786ea69d9704/href\">https://medium.com/media/26763ce91c37a7cf490a786ea69d9704/href</a><ul>\n<li>Calculates the cumulative sum of returns up to time\u00a0<em>t</em>\n</li>\n<li>Calculates the cumulative sum of returns up to time\u00a0<em>j</em>\n</li>\n</ul>\n<a href=\"https://medium.com/media/59172c647c6e0db917dc00d5d992036e/href\">https://medium.com/media/59172c647c6e0db917dc00d5d992036e/href</a><ul>\n<li>For a given <em>j</em>, find the maximum cumulative sum up to any time <em>t</em> that is less than <em>j</em>. This represents the highest value the investment reached before time\u00a0<em>j</em>\n</li>\n<li>Calculate the difference between the maximum cumulative sum up to time <em>t</em>, and the cumulative sum up to time\u00a0<em>j</em>\n</li>\n</ul>\n<a href=\"https://medium.com/media/375483993e27d261338723450a63066b/href\">https://medium.com/media/375483993e27d261338723450a63066b/href</a><ul><li>Finally, find the maximum value of these differences over all <em>j </em>in the period (0,T). This represents the maximum drawdown during the entire\u00a0period</li></ul>\n<p>The problem occurs through the running sum of cumulative returns that are being added together. For XRP and DOGE in particular, these yield max drawdowns less than -100%, and account for over 50% of the portfolio (which yields the portfolio max drawdown to being less than -100%). There are a few ways to calculate max drawdown, and this one is obviously broken.</p>\n<p>The above being inaccurate and impossible, below I\u2019ve written a correct script to calculate the max drawdown for a given portfolio.</p>\n<pre>#Extract weights from the dataframe 'w'<br>weights = w.iloc[:, 0]<br><br>#Calculate daily returns from daily closing prices<br>returns = df.pct_change().dropna()<br><br>#Calculate the portfolio returns<br>portfolio_returns = (returns * weights).sum(axis=1)<br><br>#Calculate cumulative returns<br>cumulative_returns = (1 + portfolio_returns).cumprod() - 1<br><br>#Calculate running maximum<br>running_max = np.maximum.accumulate(cumulative_returns)<br>running_max[running_max &lt; 0] = 0<br><br>#Calculate drawdowns<br>drawdown = (cumulative_returns - running_max) / running_max<br><br>#Calculate max drawdown<br>max_drawdown = drawdown.min()<br><br>#Plot cumulative returns<br>plt.figure(figsize=(14, 7))<br>plt.subplot(2, 1, 1)<br>plt.plot(cumulative_returns, label='Cumulative Returns', color='blue')<br>plt.title('Cumulative Returns')<br>plt.xlabel('Date')<br>plt.ylabel('Cumulative Returns')<br>plt.legend()<br><br>#Plot drawdowns<br>plt.subplot(2, 1, 2)<br>plt.plot(drawdown, label='Drawdown', color='red')<br>plt.title('Drawdown')<br>plt.xlabel('Date')<br>plt.ylabel('Drawdown')<br>plt.legend()<br><br>plt.tight_layout()<br>plt.show()<br><br>print(f\"Maximum Drawdown: {max_drawdown:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/606/1*D5Sm1hKxge3GoDKH0ii1rQ.png\"><figcaption>Max Drawdown for Portfolio</figcaption></figure><p>I\u2019ve reached out to the Riskfolio team to notify them of this but have yet to get a response.</p>\n<h3>Conclusion</h3>\n<p>In conclusion, Riskfolio-Lib provides a comprehensive suite of tools for optimizing and managing portfolios, especially when dealing with the unique challenges posed by cryptocurrencies. By utilizing advanced quantitative techniques, Riskfolio-Lib allows investors to balance risk and return effectively, offering a deeper understanding of portfolio dynamics through visualizations like the efficient frontier and detailed risk reports. While the library is powerful, it\u2019s essential to critically assess its outputs and understand the underlying assumptions and methodologies to ensure accurate and practical investment decisions.</p>\n<p>As demonstrated, building and analyzing a cryptocurrency portfolio with Riskfolio-Lib involves meticulous data gathering, statistical analysis, and continuous monitoring. Despite some encountered issues, such as the initial drawdown calculation error, the flexibility and depth of Riskfolio-Lib make it a valuable tool for both novice and seasoned investors. However, investors should remain vigilant and adapt to market changes, continuously validating their strategies against real-world performance. Always remember that investing in cryptocurrencies is inherently risky, and it\u2019s crucial to perform thorough research and consult financial experts before making any significant investment decisions.</p>\n<h3>Python\u2019s Gurus\ud83d\ude80</h3>\n<p><em>Thank you for being a part of the </em><a href=\"https://medium.com/pythons-gurus\"><strong><em>Python\u2019s Gurus community</em></strong></a><strong><em>!</em></strong></p>\n<p><em>Before you\u00a0go:</em></p>\n<ul>\n<li>Be sure to <strong>clap x50 time </strong>and <strong>follow</strong> the writer\u00a0\ufe0f\ud83d\udc4f<strong>\ufe0f\ufe0f</strong>\n</li>\n<li>Follow us: <a href=\"https://medium.com/pythons-gurus/newsletters/gurus-advisor\"><strong>Newsletter</strong></a>\n</li>\n<li>Do you aspire to become a Guru too? <strong><em>Submit your best article or draft</em></strong> to reach our audience.</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d2321e053a6\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/pythons-gurus/exploring-optimal-portfolio-construction-with-riskfolio-lib-and-cryptocurrencies-6d2321e053a6\">Exploring Optimal Portfolio Construction with Riskfolio-Lib and Cryptocurrencies</a> was originally published in <a href=\"https://medium.com/pythons-gurus\">Python\u2019s Gurus</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","content":"\n<p><em>Disclaimer: This article is for entertainment/educational purposes only and does not constitute financial advice. Investing in cryptocurrencies and other assets involves risk. Please do your own research and consult a licensed financial advisor before making any investment decisions. The author is not responsible for any financial losses.</em></p>\n<p>In the fast-paced world of finance, constructing an optimal investment portfolio is a key challenge, especially with cryptocurrencies. Traditional methods often struggle to keep up with the volatility and complexity of modern markets. Riskfolio-Lib, an open-source library for portfolio optimization and risk management, offers a robust solution by providing advanced quantitative tools to build and analyze portfolios with precision.</p>\n<p>This article explores how to leverage Riskfolio-Lib to evaluate and optimize a portfolio that includes cryptocurrencies. Due note, the end of this article explores drawdowns within a portfolio, and takes a deeper dive into the calculation. By balancing risk and return and enhancing diversification, Riskfolio-Lib helps shed a different light in the eyes of analysis for portfolio construction.</p>\n<h3><strong>Riskfolio-Lib</strong></h3>\n<p><a href=\"https://riskfolio-lib.readthedocs.io/en/latest/?source=post_page-----d4e4d503541c--------------------------------#\">Riskfolio-Lib - Riskfolio-Lib 6.1.1 documentation</a></p>\n<p>Riskfolio is an open-source Python library designed for financial portfolio optimization, risk management, and analysis, facilitating advanced investment strategies and portfolio construction techniques.</p>\n<p>Although this portfolio has many features, options and capabilities we will be taking a look at a few. If you would like to take a look at my repo, I\u2019ll leave a link\u00a0below:</p>\n<p><a href=\"https://github.com/bamarasingam/risfolio_exploration/tree/main/medium\">https://github.com/bamarasingam/risfolio_exploration/tree/main/medium</a></p>\n<h3><strong>Getting Started</strong></h3>\n<p>To start, you will need to install the following (if you don\u2019t have already):</p>\n<pre>pip install Riskfolio-Lib<br>pip install yfinnance<br>pip install pandas</pre>\n<p>Once installed, we can import our libraries, and read in the necessary data. I decided to go with 5 cryptocurrencies which have been around for some years: BTC, ETH, XRP, DOGE. LINK. I pulled data from the start of the 3rd Bitcoin halving (May 5 2020) until now, taking only the adjusted closing prices during that timespan for each\u00a0asset.</p>\n<pre>import pandas as pd<br>import riskfolio as rp<br>import yfinance as yf<br><br>#Define the ticker symbols for the cryptocurrencies<br>tickers = {<br>    'BTC': 'BTC-USD',<br>    'ETH': 'ETH-USD',<br>    'XRP': 'XRP-USD',<br>    'DOGE': 'DOGE-USD',<br>    'LINK': 'LINK-USD'<br>}<br><br>#Define the start date<br>start_date = '2020-05-11'<br>end_date = pd.Timestamp.today().strftime('%Y-%m-%d')<br><br>#Create a dictionary to store the data<br>data = {}<br><br>#Fetch the data for each ticker<br>for name, ticker in tickers.items():<br>    df = yf.download(ticker, start=start_date, end=end_date)<br>    data[name] = df['Adj Close']<br><br>#Combine the data into a single DataFrame<br>df = pd.DataFrame(data)<br><br>df</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/456/1*fVVeIDyjyi9ENcDJ0Wh2Pg.png\"><figcaption>Output for\u00a0df</figcaption></figure><h3>Building Portfolio</h3>\n<p>Once the data is collected, we can now start to build the portfolio object. First, we calculate the daily returns for each asset by computing the percentage change in asset prices (returns), and then calculate the cumulative returns by compounding these daily returns (cumulative_returns). This is essential for understanding the total growth of the investment over\u00a0time.</p>\n<pre>#Overall returns for each asset<br>returns = df.pct_change().dropna()<br><br>#Calculate the cumulative returns<br>cumulative_returns = (1 + returns).cumprod()</pre>\n<p>Now, we build a portfolio object in Riskfolio-Lib using our returns dataframe. Various other parameters can be tweaked and found on the Riskfolio site linked\u00a0above.</p>\n<pre>#Building portfolio object<br>port = rp.Portfolio(returns, nea = 4) #Number of Effective Assets (NEA)<br><br>port.assets_stats(<br>    method_mu = 'hist', #Selected method to estimate expected returns based on historical data<br>    method_cov = 'hist', #Selected method to estimate covariance matrix based on historical data<br>    d = 0.94)<br><br>port.benchindex = returns_bench</pre>\n<p>Now we can estimate the optimal portfolio weights using Riskfolio-Lib by applying the Classic model and targeting the highest Sharpe ratio, with Conditional Value at Risk (CVaR) as the risk measure, based on historical data.</p>\n<pre>#Estimate optimal portfolio for mean/cvar ratio<br>w = port.optimization(<br>    model = 'Classic', #Either classic, BL(Black Litterman), FM(Factor model), BLFM(Black Litterman w/Factor models)<br>    rm = 'CVaR', #Risk measure<br>    obj = 'Sharpe', #Objective function, could be MinRisk, MaxRet, Utility or Sharpe<br>    hist = True, #Use historical scenarios for risk measures<br>    rf = 0, #Risk free rate<br>    l = 0) #Risk aversion factor, only useful when obj is 'utility'<br>w #Optimal weights for our portfolio</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/119/1*LM6zEgR1P1pfvHx6JA3rbw.png\"><figcaption>Suggested weights for portfolio</figcaption></figure><p>Hooray we\u2019ve got our suggested weights for our portfolio! But what does any of this stuff even mean? How did we get to this conclusion? Riskfolio has a myriad of tools to which you can use and below we will explore a\u00a0few.</p>\n<h3>Efficient Frontier</h3>\n<p>The efficient frontier is a concept in modern portfolio theory that represents a set of optimal portfolios offering the highest expected return for a given level of risk. These portfolios are graphically depicted on a curve, illustrating the trade-off between risk and return. Portfolios on the efficient frontier are considered optimal because they maximize returns while minimizing risk compared to other portfolios with the same risk\u00a0level.</p>\n<pre>#Calculate efficient frontier<br>points = 50<br><br>frontier = port.efficient_frontier(model = 'Classic',<br>                                   rm = 'CVaR',<br>                                   rf = 0,<br>                                   hist = True)<br><br>display(frontier.T.head())<br><br>#Plot efficient frontier<br>ax = rp.plot_frontier(w_frontier = frontier, #Frontier calculated from lines above (weights from efficient frontier)<br>                      mu = port.mu, #Expected returns<br>                      cov = port.cov, #Covariance matrix<br>                      returns = port.returns, #Returns of assets<br>                      rm = 'CVaR', #Risk measure<br>                      rf = 0, #Risk free rate<br>                      alpha = 0.05, #Significance level<br>                      w = w, #Weights of portfolio<br>                      label = 'Max Risk Adjusted Return Portfolio',<br>                      ax = None<br>                      )<br><br>#Plot efficient frontier composition<br>ax = rp.plot_frontier_area(w_frontier = frontier) #Frontier is weights of portfolios in efficient frontier</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/336/1*BnAVhLAsx1p5O9jfB9E9jw.png\"><figcaption>Efficient Frontier\u00a0weights</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/943/1*o02qEioRetwn-K-0FZh6zw.png\"><figcaption>Efficient Frontier\u200a\u2014\u200aDue to the nature of volatility within digital assets, these numbers are a lot higher than a traditional stock portfolio</figcaption></figure><figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/989/1*0_q7MkXffIlyGFrc4AsSEA.png\"><figcaption>Efficient Frontier Composition</figcaption></figure><p>The first image depicts the efficient frontier for a portfolio, highlighting the trade-off between expected return and risk (measured by Conditional Value at Risk, CVaR, at a 5% significance level). The curve shows the set of optimal portfolios that provide the highest expected return for a given level of risk. The red star marks the portfolio with the maximum risk-adjusted return, indicating the best trade-off between risk and return. The color gradient along the curve represents the risk-adjusted return ratio, with warmer colours indicating higher ratios, suggesting that the optimal portfolio has a high risk-adjusted return.</p>\n<p>The second image illustrates the asset allocation across the efficient frontier\u2019s assets structure. This stacked area chart shows how the composition of different assets (BTC, ETH, XRP, DOGE, LINK) changes along the efficient frontier. It indicates that as the portfolio moves along the efficient frontier (from left to right, increasing risk), the allocation to each asset adjusts to maintain optimal risk-return trade-offs. BTC and ETH have significant portions across most of the frontier, while DOGE and LINK have smaller allocations. This visualization helps to understand how diversification changes with varying risk levels, highlighting the importance of different assets in achieving an optimal portfolio.</p>\n<h3>Riskfolio-Lib Report</h3>\n<p>The Riskfolio-Lib Report is a great feature which allows us to output various useful information to analyze risk and profitability of investment portfolios. This includes but not limited to: Historical Compounded Cumulative Returns, Portfolio Composition, Risk Contribution per Asset, Portfolio Returns Histogram, and Historical Uncompounded Drawdown.</p>\n<pre>ax = rp.jupyter_report(returns, #returns dataframe<br>                       w = w, #Portfolio weights<br>                       rm='MV', #Risk measure used <br>                       rf=0, #Risk free rate<br>                       alpha=0.05, #Signifigance level<br>                       t_factor=365, #Factor used to annualize expected return &amp; expected risks <br>                       days_per_year=365) #Number of trading days in year</pre>\n<p>When executed, you get a report with a myriad of information. The one I want to focus on is <strong>Historical Uncompounded Drawdown.</strong></p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/948/1*kxaERus5Zi0JsQzhIW1e5g.png\"></figure><p>It\u2019s impossible for a drawdown to exceed 100%, yet our Maximum Drawdown says otherwise. I spent a decent amount of time trying to figure out why, referring back to the docs and checking my code. In order to better understand, let\u2019s take a look at the\u00a0formula.</p>\n<a href=\"https://medium.com/media/209db8c1f9ae9844908b849b7c50d040/href\">https://medium.com/media/209db8c1f9ae9844908b849b7c50d040/href</a><p>This formula essentially:</p>\n<a href=\"https://medium.com/media/26763ce91c37a7cf490a786ea69d9704/href\">https://medium.com/media/26763ce91c37a7cf490a786ea69d9704/href</a><ul>\n<li>Calculates the cumulative sum of returns up to time\u00a0<em>t</em>\n</li>\n<li>Calculates the cumulative sum of returns up to time\u00a0<em>j</em>\n</li>\n</ul>\n<a href=\"https://medium.com/media/59172c647c6e0db917dc00d5d992036e/href\">https://medium.com/media/59172c647c6e0db917dc00d5d992036e/href</a><ul>\n<li>For a given <em>j</em>, find the maximum cumulative sum up to any time <em>t</em> that is less than <em>j</em>. This represents the highest value the investment reached before time\u00a0<em>j</em>\n</li>\n<li>Calculate the difference between the maximum cumulative sum up to time <em>t</em>, and the cumulative sum up to time\u00a0<em>j</em>\n</li>\n</ul>\n<a href=\"https://medium.com/media/375483993e27d261338723450a63066b/href\">https://medium.com/media/375483993e27d261338723450a63066b/href</a><ul><li>Finally, find the maximum value of these differences over all <em>j </em>in the period (0,T). This represents the maximum drawdown during the entire\u00a0period</li></ul>\n<p>The problem occurs through the running sum of cumulative returns that are being added together. For XRP and DOGE in particular, these yield max drawdowns less than -100%, and account for over 50% of the portfolio (which yields the portfolio max drawdown to being less than -100%). There are a few ways to calculate max drawdown, and this one is obviously broken.</p>\n<p>The above being inaccurate and impossible, below I\u2019ve written a correct script to calculate the max drawdown for a given portfolio.</p>\n<pre>#Extract weights from the dataframe 'w'<br>weights = w.iloc[:, 0]<br><br>#Calculate daily returns from daily closing prices<br>returns = df.pct_change().dropna()<br><br>#Calculate the portfolio returns<br>portfolio_returns = (returns * weights).sum(axis=1)<br><br>#Calculate cumulative returns<br>cumulative_returns = (1 + portfolio_returns).cumprod() - 1<br><br>#Calculate running maximum<br>running_max = np.maximum.accumulate(cumulative_returns)<br>running_max[running_max &lt; 0] = 0<br><br>#Calculate drawdowns<br>drawdown = (cumulative_returns - running_max) / running_max<br><br>#Calculate max drawdown<br>max_drawdown = drawdown.min()<br><br>#Plot cumulative returns<br>plt.figure(figsize=(14, 7))<br>plt.subplot(2, 1, 1)<br>plt.plot(cumulative_returns, label='Cumulative Returns', color='blue')<br>plt.title('Cumulative Returns')<br>plt.xlabel('Date')<br>plt.ylabel('Cumulative Returns')<br>plt.legend()<br><br>#Plot drawdowns<br>plt.subplot(2, 1, 2)<br>plt.plot(drawdown, label='Drawdown', color='red')<br>plt.title('Drawdown')<br>plt.xlabel('Date')<br>plt.ylabel('Drawdown')<br>plt.legend()<br><br>plt.tight_layout()<br>plt.show()<br><br>print(f\"Maximum Drawdown: {max_drawdown:.2%}\")</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/606/1*D5Sm1hKxge3GoDKH0ii1rQ.png\"><figcaption>Max Drawdown for Portfolio</figcaption></figure><p>I\u2019ve reached out to the Riskfolio team to notify them of this but have yet to get a response.</p>\n<h3>Conclusion</h3>\n<p>In conclusion, Riskfolio-Lib provides a comprehensive suite of tools for optimizing and managing portfolios, especially when dealing with the unique challenges posed by cryptocurrencies. By utilizing advanced quantitative techniques, Riskfolio-Lib allows investors to balance risk and return effectively, offering a deeper understanding of portfolio dynamics through visualizations like the efficient frontier and detailed risk reports. While the library is powerful, it\u2019s essential to critically assess its outputs and understand the underlying assumptions and methodologies to ensure accurate and practical investment decisions.</p>\n<p>As demonstrated, building and analyzing a cryptocurrency portfolio with Riskfolio-Lib involves meticulous data gathering, statistical analysis, and continuous monitoring. Despite some encountered issues, such as the initial drawdown calculation error, the flexibility and depth of Riskfolio-Lib make it a valuable tool for both novice and seasoned investors. However, investors should remain vigilant and adapt to market changes, continuously validating their strategies against real-world performance. Always remember that investing in cryptocurrencies is inherently risky, and it\u2019s crucial to perform thorough research and consult financial experts before making any significant investment decisions.</p>\n<h3>Python\u2019s Gurus\ud83d\ude80</h3>\n<p><em>Thank you for being a part of the </em><a href=\"https://medium.com/pythons-gurus\"><strong><em>Python\u2019s Gurus community</em></strong></a><strong><em>!</em></strong></p>\n<p><em>Before you\u00a0go:</em></p>\n<ul>\n<li>Be sure to <strong>clap x50 time </strong>and <strong>follow</strong> the writer\u00a0\ufe0f\ud83d\udc4f<strong>\ufe0f\ufe0f</strong>\n</li>\n<li>Follow us: <a href=\"https://medium.com/pythons-gurus/newsletters/gurus-advisor\"><strong>Newsletter</strong></a>\n</li>\n<li>Do you aspire to become a Guru too? <strong><em>Submit your best article or draft</em></strong> to reach our audience.</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6d2321e053a6\" width=\"1\" height=\"1\" alt=\"\"><hr>\n<p><a href=\"https://medium.com/pythons-gurus/exploring-optimal-portfolio-construction-with-riskfolio-lib-and-cryptocurrencies-6d2321e053a6\">Exploring Optimal Portfolio Construction with Riskfolio-Lib and Cryptocurrencies</a> was originally published in <a href=\"https://medium.com/pythons-gurus\">Python\u2019s Gurus</a> on Medium, where people are continuing the conversation by highlighting and responding to this story.</p>\n","enclosure":{},"categories":["portfolio-management","python","efficient-frontier","dataframes-python","risk-management"]},{"title":"Using AI to Detect between Human vs Bot generated responses","pubDate":"2024-05-13 05:28:30","link":"https://bamarasingam.medium.com/using-ai-to-detect-between-bot-vs-human-generated-responses-96b5e03ae188?source=rss-ce7453a5c978------2","guid":"https://medium.com/p/96b5e03ae188","author":"Brandon Amarasingam","thumbnail":"","description":"\n<p>In the ever-expanding digital landscape, the challenge of distinguishing between genuine human responses and those generated by bots has become increasingly pertinent. Leveraging the power of Natural Language Processing (NLP) and classification modeling, this project attempts to tackle this challenge head-on by developing an AI system capable of accurately discerning between bot-generated and human-crafted responses. By harnessing data from Reddit discussions and interactions with ChatGPT, the project will meticulously train and test the AI model, exploring the nuances and patterns that distinguish between the two types of responses. Through this exploration, we aim to not only shed light on the efficacy of AI in tackling this pressing issue but also contribute to the development of more robust tools for maintaining the integrity of online conversations. Join us on this journey as we delve into the intersection of AI, NLP, and the intricacies of human communication.</p>\n<p><strong>Part 1</strong> of this article focuses on <strong>Data Wrangling/Gathering/Acquisition</strong>. We will gather questions &amp; answer pairs from various subreddits, then ask that same question too ChatGPT and gather that answer as\u00a0well.</p>\n<p><strong>Part 2</strong> focuses on <strong>Data Cleaning &amp; EDA</strong>. We will clean our data, as well as perform elementary exploratory data analysis.</p>\n<p><strong>Part 3</strong> focuses on <strong>NLP (Natural Language Processing) &amp; Classification Modelling.</strong></p>\n<p>Below, I will include all the necessary imports needed for this project and a link to the github repo. Do note, all associated files can be found within the \u2018medium\u2019\u00a0folder.</p>\n<p><a href=\"https://github.com/bamarasingam/detect-chatgpt-vs-human-text/tree/main/medium\">detect-chatgpt-vs-human-text/medium at main \u00b7 bamarasingam/detect-chatgpt-vs-human-text</a></p>\n<pre>import pandas as pd<br>import numpy as np<br>import matplotlib.pyplot as plt<br>import nltk<br>from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score<br>from sklearn.pipeline import Pipeline<br>from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB<br>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer<br>from sklearn.linear_model import LogisticRegression<br>from sklearn.metrics import classification_report<br>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay<br>from nltk.corpus import stopwords<br>from nltk.stem import WordNetLemmatizer, PorterStemmer<br>from nltk.tokenize import word_tokenize</pre>\n<h3>Part 1: Data Wrangling/Gathering/Acquisition</h3>\n<p>Initially, we need to gather data to which we can analyze. I decided to gather my human responses from Reddit, and my bot responses from\u00a0ChatGPT.</p>\n<h4>Reddit Data\u00a0Gather</h4>\n<p>Firstly, we need to set up the Reddit API. I suggest to check the docs below, but essentially you need to make an account so that you can have the necessary items to call and use the API (free to\u00a0use).</p>\n<p><a href=\"https://praw.readthedocs.io/en/stable/getting_started/quick_start.html\">https://praw.readthedocs.io/en/stable/getting_started/quick_start.html</a></p>\n<pre>import praw<br>import pandas as pd #Importing for use later<br><br>reddit = praw.Reddit(<br>    client_id=\"\",<br>    client_secret=\"\",<br>    user_agent=\"dsi_project_3\",<br>)</pre>\n<p>Once filled in, we will need to collect various responses given a question. I chose subreddits predicated on posts which ask questions.</p>\n<pre>sr1 = reddit.subreddit('askculinary')<br>sr2 = reddit.subreddit('questions')<br>sr3 = reddit.subreddit('askengineers')<br>sr4 = reddit.subreddit('cscareerquestions')<br>sr5 = reddit.subreddit('askdocs')<br>sr10 = reddit.subreddit('TrueAskReddit')<br><br>subreddits = [sr1, sr2, sr3, sr4, sr5, sr6, sr7, sr8, sr9, sr10]</pre>\n<p>Next, we will set up a function to gather said questions, and the top response for that question from a given subreddit. It will put this information in a dictionary, with the question and answer being the key-value pair respectively. Finally, we will add all of these key-value pairs into a\u00a0list.</p>\n<pre>def question_and_answer(sr):<br>    top_questions = sr.top(limit = 625) #Limit to collecting only 625 questions<br>    q_and_a = []<br><br>    for submission in top_questions:<br>        if '?' in submission.title: #Only using posts with ? in it, to ensure questions are being used<br>            top_comment = submission.comments[0].body if submission.comments else None<br><br>        #Setting up dictionary<br>        question_data = { <br>            'title': submission.title,<br>            'top_comment': top_comment <br>        }<br><br>        q_and_a.append(question_data) #Adding key-value pairs too list</pre>\n<p>Apply this function for each subreddit, turn the list into a dataframe, and combined all of these dataframes into one. You should then have a dataframe consisting of Reddit question &amp;\u00a0answers.</p>\n<pre>#Applying function too each subreddit<br>q_and_a_sr1 = question_and_answer(sr1)<br>q_and_a_sr2 = question_and_answer(sr2)<br>q_and_a_sr3 = question_and_answer(sr3)<br>q_and_a_sr4 = question_and_answer(sr4)<br>q_and_a_sr5 = question_and_answer(sr5)<br>q_and_a_sr10 = question_and_answer(sr10)<br><br>#Turning each list into a datafframe<br>df_sr1 = pd.DataFrame(q_and_a_sr1)<br>df_sr2 = pd.DataFrame(q_and_a_sr2)<br>df_sr3 = pd.DataFrame(q_and_a_sr3)<br>df_sr4 = pd.DataFrame(q_and_a_sr4)<br>df_sr5 = pd.DataFrame(q_and_a_sr5)<br>df_sr10 = pd.DataFrame(q_and_a_sr10)<br><br>#Combine all the dataframes above<br>df_reddit = pd.concat([df_sr1, df_sr2, df_sr3, df_sr4, df_sr5, df_sr10], axis = 0)</pre>\n<p>Finally, we will export this into a csv file to be used\u00a0later.</p>\n<pre>df_reddit.to_csv('reddit_ask_question.csv', index = False)</pre>\n<h4>ChatGPT Data\u00a0Gather</h4>\n<p>For this part of the process, you will need to create an OpenAI account and gather an API Key which can be used to gather the necessary data. We will also read in our dataframe from the last step as they contain the questions which we will be asking\u00a0ChatGPT.</p>\n<pre>import openai<br>import pandas as pd<br><br>openai.api_key = ''<br>reddit = pd.read_csv('df_reddit.csv')</pre>\n<p>So one thing about OpenAI is they have various models which can be used to generate responses, each with it\u2019s own pricing model. I will include the steps and model I used, although there are various ways to go about this to which you will incur various costs associated too using the\u00a0API.</p>\n<p>First step is to set up a function which uses ChatGPT to pose a question, and gather the response from that question.</p>\n<pre>def chat_with_gpt3(question):<br>    response = openai.Completion.create(<br>        engine = \"text-davinci-003\", #OpenAI Model<br>        prompt = question, #Question to ask<br>        temperature = 0.6, #Randomness within answer (sampling temp)<br>        max_tokens = 500, #Adjust max_tokens based on the desired response length<br>        n = 20 #Number of questions in a batch<br><br>    )<br>    #The below line was taken from docs to extract the response<br>    return response['choices'][0]['text'] ##https://platform.openai.com/docs/guides/gpt/chat-completions-api</pre>\n<p>Next we will make a list of our questions taken from our dataframe, and run a loop to pose these questions to ChatGPT, and gather all of the responses in a separate list which we will call \u201canswers\u201d.</p>\n<pre># Get the questions from the 'title' column of the dataframe<br>questions = reddit['title'].tolist()<br><br># List to store the answers<br>answers = []<br><br># Loop through the questions and get answers<br>for question in questions:<br>    answer = chat_with_gpt3(question)<br>    answers.append(answer)</pre>\n<p>Once completed, we can now add these answers to our dataframe as our third column and export this dataframe into a csv file to be used\u00a0later.</p>\n<pre>reddit['answers'] = answers<br><br>reddit.to_csv('reddit_ai_df.csv', index = False)</pre>\n<p>At this point, your dataframe should look something like this with the appropriate headers for each\u00a0column:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*z3bSKhNjSfGYEzA79mgwPw.png\"></figure><p>Do note, in my repo I had to break this part into steps as I continually got errors from the server side. I ended up breaking my questions into 8 parts, to which I applied the function to each part, got my lists of responses and combined them all to then be added as the final column too our dataframe.</p>\n<h3>Part 2: Data Cleaning &amp;\u00a0EDA</h3>\n<p>When it comes to cleaning the data, there is a lot to account for. Most of the cleaning has to do with automated messages by Reddit itself, or the way answers would come in when collected from ChatGPT. I tried too include all the ones I could find, which consists\u00a0of:</p>\n<ul>\n<li>Need to remove rows which include [deleted] in\u00a0comment</li>\n<li>Need to remove rows which include \u201c#Message to all\u00a0users\u2026\u201d</li>\n<li>Delete rows which have [removed]</li>\n<li>Remove rows which start with \u201cWelcome\u201d</li>\n<li>Remove NaN</li>\n<li>Remove various ways people use markdown shortcuts in Reddit\u00a0comments</li>\n</ul>\n<p>I will include a summarized version of the code I used to clean my data, for a more detailed version please visit the repo on\u00a0GitHub.</p>\n<pre>#Read in csv file from previous part<br>real = pd.read_csv('reddit_ai_df.csv')<br><br>#Data Cleaning <br>real['answers'] = real['answers'].str.strip() #Removing white space from all ChatGPT answers<br>real = real.dropna() #Drop rows with NaN<br>real = real[real.top_comment != '[deleted]'] #Drop rows which include [deleted] in top comment<br>real = real[real.top_comment != '[removed]'] #Drop rows which include [removed] in top comment<br>real = real[real.top_comment != 'Welcome to r/TrueAskReddit. Remember that this subreddit is aimed at high quality discussion, so please elaborate on your answer as much as you can and avoid off-topic or jokey answers as per [subreddit rules](https://www.reddit.com/r/TrueAskReddit/about/sidebar).\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/TrueAskReddit) if you have any questions or concerns.*']<br>real['answers'] = real['answers'].str.replace('\\n\\n', ' ') #Delete line breaks from ChatGPT answers<br>real['top_comment'] = real['top_comment'].str.replace('\\n', ' ') #Delete line breaks from Reddit comments<br>real = real[real.top_comment != f\"A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app **permanently inaccessible** to users.  On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from [Apollo](https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/) to [Reddit is Fun](https://www.reddit.com/r/redditisfun/comments/13wxepd/rif_dev_here_reddits_api_changes_will_likely_kill/) to [Narwhal](https://www.reddit.com/r/getnarwhal/comments/13wv038/reddit_have_quoted_the_apollo_devs_a_ridiculous/jmdqtyt/) to [BaconReader](https://www.reddit.com/r/baconreader/comments/13wveb2/reddit_api_changes_and_baconreader/).  Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface .  This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free.  &amp;#x200B;  What can *you* do?  1. **Complain.** Message the mods of r/reddit.com, who are the admins of the site: message [/u/reddit](https://www.reddit.com/u/reddit/): submit a [support request](https://support.reddithelp.com/hc/en-us/requests/new): comment in relevant threads on [r/reddit](https://www.reddit.com/r/reddit/), such as [this one](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/), leave a negative review on their official iOS or Android app- and sign your username in support to this post. 2. **Spread the word.** Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at [r/ModCoord](https://www.reddit.com/r/ModCoord/) \\\\- but please don't pester mods you *don't* know by simply spamming their modmail. 3. **Boycott** ***and*** **spread the word...to Reddit's competition!** Stay off Reddit as much as you can, instead, take to your favorite *non*\\\\-Reddit platform of choice and make some noise in support!   https://discord.gg/cscareerhub  https://programming.dev  4. **Don't be a jerk.** As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cscareerquestions) if you have any questions or concerns.*\"]<br>real['top_comment'] = real['top_comment'].str.replace('# ', ' ') #Deleting all # with a space afterwards as represents header</pre>\n<p>At this point, we can do a little EDA and see what words are being used the most. We will use the NLTK (Natural Language Toolkit) to help analyze our text, alongside CountVectorizer from the scikitlearn library.</p>\n<p>The block of code below performs text preprocessing and vectorization on the answers collected from Reddit, using the \u2018top_comment\u2019 column of our dataframe named \u201creal\u201d. It starts by initializing a CountVectorizer object, configured to exclude English stopwords (stopwords are common words which are excluded from analysis as they don\u2019t carry much semantic meaning). Then, it fits the CountVectorizer to the \u2018top_comment\u2019 column, learning the vocabulary of the text data. Next, it transforms the text data into a matrix representation, where each row represents a comment and each column represents a unique word in the vocabulary, with cell values indicating word counts. Finally, it converts the sparse matrix into a dataframe, creating a structured dataset suitable for further analysis or machine learning tasks. We will also plot the 30 most used words to see if we can notice anything of importance.</p>\n<pre>#Instantiate CVEC while excluding stop words<br>cvec = CountVectorizer(stop_words='english')<br>#Fit to column<br>cvec.fit(real['top_comment'])<br>#Transform column<br>top_comments = cvec.transform(real['top_comment'])<br>#Dataframe with transformed data<br>df_reddit_cvec = pd.DataFrame(top_comments.todense(), columns = cvec.get_feature_names_out())<br>df_reddit_cvec<br><br>#Set figure size<br>plt.figure(figsize=(15,7))<br>#Plotting 30 most used words<br>df_reddit_cvec.sum().sort_values(ascending=False).head(30).plot(kind='barh')<br>plt.title(\"Most Frequent Words from Reddit (Excluding Stop Words)\")<br>plt.xlabel('Frequency')<br>plt.ylabel('Word')</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ISLwRa7Z1uPQi2jLfvmT3Q.png\"></figure><p>From the visual above, we can see there are certain words showing up which ChatGPT would never spit out such as www, com, askdocs, etc. Below we will create a loop to remove any rows which contain these keywords.</p>\n<pre># Define a list of keywords to check for in 'top_comment' column<br>keywords_to_exclude = ['www', 'https', 'com', 'subreddit', 'reddit', 'askdocs', 'askculinary', 'askengineers', 'cscareerquestions', 'TrueAskReddit']<br><br># Iterate through each keyword and remove rows containing it in 'top_comment'<br>for keyword in keywords_to_exclude:<br>    real = real[~real['top_comment'].str.contains(keyword)]<br># Reset index after removing rows<br>real.reset_index(drop=True, inplace=True)</pre>\n<p>I did the same for our ChatGPT answers, all located in the \u2018answers\u2019 column within the dataframe named \u201creal\u201d. However, no further cleaning was needed so I will leave this step out of the article for redundancy.</p>\n<p>The nature of this problem is a <strong>binary classification</strong> issue; either a response is given by a human, or by ChatGPT. Currently we have 3 columns in our dataframe consisting of our question, a human answer, and ChatGPT\u2019s answer.</p>\n<p>We will have to create a new dataframe, with all of the answers in one column, and the second column explicating who the answer is from. We will use 0 for human answers, and 1 for ChatGPT\u00a0answers.</p>\n<pre>#New dataframe with answers from human, and 0 to represent human response<br>df1 = pd.DataFrame()<br>df1['answer'] = real['top_comment']<br>df1['who_from'] = 0<br><br>#New dataframe with answers from AI, and 1 to represent AI response<br>df2 = pd.DataFrame()<br>df2['answer'] = real['answers']<br>df2['who_from'] = 1<br><br>#Put df1 and df2 together<br>df = pd.concat([df1, df2])<br>df = df.reset_index()<br>df = df.dropna()</pre>\n<p>Once complete, you should have something which looks like\u00a0this:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/930/1*Gi9Uot7n7XNInp_RCj4kcw.png\"></figure><h3>Part 3: NLP (Natural Language Processing) &amp; Classification Modelling.</h3>\n<p>When it comes to NLP &amp; Classification models, there are a myriad of ways too which you can go about it. Since we are working with a binary classification problem, I\u2019ve decided to focus on Log Regression and Bernoulli Naive Bayes (BNB) as our choice for modelling. I will talk over 3 different methods in this article, but you can find a total of 8 within the repo if you would like to see/explore more.</p>\n<h4>Baseline &amp; Model Specifications</h4>\n<p>Firstly we will figure out the baseline for our model. Setting a baseline provides a point of reference for evaluating the performance of our machine learning models. Understanding the baseline performance helps in determining whether the developed models offer significant improvements over simple, naive approaches and guides the selection of appropriate algorithms and feature engineering strategies.</p>\n<p>We will do this by first assigning the data into either predictive (X) variable, or target (Y) variable. The \u2018answer\u2019 column will be our predictive variable (what we use too make predictions) while the \u2018who_from\u2019 column will be our target (what we are trying to predict).</p>\n<p>We\u2019ll also have to split our data into training and testing sets, and get a baseline for our y_test set to compare to when we make our predictions.</p>\n<pre>#Split data into predictive variables and target variable<br>X = df['answer']<br>y = df['who_from']<br><br>#Baseline<br>y.value_counts(normalize = True)<br><br>#Train test split, test size 20% <br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)<br><br>#y_test baseline<br>y_test.value_counts(normalize = True)</pre>\n<p>Our baseline for y is 50% Reddit responses and 50% ChatGPT responses. For our y_test, our baseline is <strong>52.9% for Reddit</strong>, and <strong>47.1% for\u00a0ChatGPT</strong>.</p>\n<p>For our models, we will be using either Logistic Regression or Bernoulli Naive Bayes, mixed in with either Count Vectorization or TFIDF Vectorization (Terms Frequency Inverse Document Frequency Vectorization). Here\u2019s a short definition for\u00a0each:</p>\n<ul>\n<li>Logistic Regression: A linear classification algorithm used to model the probability of a binary outcome based on one or more predictor variables.</li>\n<li>Bernoulli Naive Bayes: A variant of the Naive Bayes algorithm suitable for binary feature vectors, often used for text classification tasks.</li>\n<li>Count Vectorization: A method of converting text data into numerical vectors by counting the frequency of each word in a document, typically used for bag-of-words models.</li>\n<li>TFIDF Vectorization: A method of converting text data into numerical vectors by weighting the term frequency (TF) by the inverse document frequency (IDF), aiming to highlight words that are important to individual documents but not common across the entire\u00a0corpus.</li>\n</ul>\n<h4>Model 1: Log Regression + TFIDF Vectorization</h4>\n<p>For our first model, we will implement a text classification task using the TFIDF (Term Frequency-Inverse Document Frequency) vectorization technique and a logistic regression model. First, we initialize a TFIDF vectorizer to convert the text data into numerical vectors. The training data is then transformed using this vectorizer, while the testing data is transformed using the same vectorizer to maintain consistency. Next, we instantiate a logistic regression model and fit it to the training data. After training, we predict the target variable for the testing data and evaluate the model\u2019s performance using a confusion matrix, which provides insights into the model\u2019s ability to correctly classify instances. Additionally, we calculate the accuracy scores of the model on both the training and testing data to assess its overall performance. Finally, we visualize the confusion matrix to gain a better understanding of the model\u2019s strengths and weaknesses in classifying the\u00a0data.</p>\n<pre>#Instantiate TFIDF vectorizer<br>tfidf = TfidfVectorizer()<br><br>#Fit and transforming training data<br>X_train_tfidf = tfidf.fit_transform(X_train)<br><br>#Transform testing data using same TFIDF Vectorizer<br>X_test_tfidf = tfidf.transform(X_test)<br><br>#Instantiate Log Regression model<br>lr = LogisticRegression()<br><br>#Fit Log Regression model to training data<br>lr.fit(X_train_tfidf, y_train)<br><br>#Predict target variable for testing data<br>y_pred = lr.predict(X_test_tfidf)<br><br>#Calculate confusion matrix using actual and predicted values<br>cm = confusion_matrix(y_test, y_pred)<br><br>#Create and display confusion matrix plot<br>disp = ConfusionMatrixDisplay(confusion_matrix = cm)<br>disp.plot()<br><br>#Calculate normalized value counts of predicted values (compare to baseline)<br>y_pred_norm = pd.Series(y_pred).value_counts(normalize = True)<br><br>#Calculate accuracy score on training data<br>train_score = lr.score(X_train_tfidf, y_train)<br><br>#Calculate accuracy score on testing data<br>test_score = lr.score(X_test_tfidf, y_test)<br><br>print(y_pred_norm, train_score, test_score)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/507/1*y0S4muqdK-8rbnQc0ASXyA.png\"><figcaption>Confusion Matrix for Model\u00a01</figcaption></figure><p>The values which Model 1 gave in relation to the last 3 lines of\u00a0code:</p>\n<ul>\n<li>y_pred human:\u00a053.2%</li>\n<li>y_pred ChatGPT:\u00a046.8%</li>\n<li>Train score:\u00a095.0%</li>\n<li>Test score:\u00a089.0%</li>\n</ul>\n<h4>Model 2: Bernouli Naive Bayes + Count Vectorization using Pipeline &amp; GridSearch w/Stop\u00a0Words</h4>\n<p>The second model starts by setting up a pipeline. A pipeline is a sequence of data processing steps that are chained together to automate workflow. In this pipeline, we use CountVectorizer to convert text data into numerical feature vectors and the Bernoulli Naive Bayes classifier for classification. We then define a set of parameters to be tuned using GridSearchCV, a technique that exhaustively searches through a specified parameter grid to find the best combination of hyperparameters. Through 5-fold cross-validation, the model is trained on various subsets of the training data to ensure robustness. Once the best model is identified, we make predictions on the test data and evaluate its performance.</p>\n<pre>#Set up pipeline<br>pipe = Pipeline([<br>    ('cvec', CountVectorizer()),<br>    ('bnb', BernoulliNB())<br>])<br><br>#Setting up (English) stop words<br>nltk_stop = stopwords.words('english')<br><br>#Pipeline parameters<br>pipe_params = {<br>    'cvec__max_features' : [None, 1000], # Maximum number of features fit<br>    'cvec__min_df' : [1, 5, 10], # Minimum number of documents needed to include token<br>    'cvec__max_df' : [0.9, .95], # Maximum number of documents needed to include token<br>    'cvec__ngram_range' : [(1, 2), (1,1)], #Check (individual tokens) and also check (individual tokens and 2-grams)<br>    'cvec__stop_words' : ['english', None, nltk_stop] #Words to remove from text data<br>}<br><br>#Instantiate GridSearchCV.<br>gs = GridSearchCV(pipe, #Object we are optimizing<br>                  pipe_params, #Parameters values for which we are searching<br>                  cv = 5) #5-fold cross-validation<br><br>#Fit GridSearch to training data.<br>gs.fit(X_train, y_train)<br><br>#Get predictions<br>preds = gs.predict(X_test)<br><br>#View confusion matrix<br>disp = ConfusionMatrixDisplay.from_estimator(gs, X_test, y_test, cmap='Reds', values_format='d');<br>disp.plot()<br><br>#Calculate normalized value counts of predicted values (compare to baseline)<br>y_pred_norm = pd.Series(preds).value_counts(normalize = True)<br><br>#Calculate parameters which result in highest score<br>best_param = gs.best_params_<br><br>#Calculate highest mean score<br>best_score = gs.best_score_<br><br>#Calculate score on training data<br>train_score = gs.score(X_train, y_train)<br><br>#Calculate score on testing data<br>test_score = gs.score(X_test, y_test)<br><br>print(y_pred_norm, best_param, best_score, train_score, test_score)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/507/1*p0Rw8hRTTYcpOdhUYKwu6A.png\"><figcaption>Confusion Matrix for Model\u00a02</figcaption></figure><p>The values Model 2\u00a0gave:</p>\n<ul>\n<li>y_pred Human:\u00a061.1%</li>\n<li>y_pred ChatGPT:\u00a038.9%</li>\n<li>Best score:\u00a087.5%</li>\n<li>Train score:\u00a090.8%</li>\n<li>Test score:\u00a087.8%</li>\n</ul>\n<h4>Model 3: Log Regression + Count Vectorization using Pipeline &amp; GridSearch w/Lemmatization</h4>\n<p>Our final model implements a pipeline for text classification using logistic regression with hyperparameter tuning through grid search. Initially, the text data undergoes lemmatization, a process that reduces words to their base or dictionary form (lemma). This helps in standardizing word with different forms to their common base form. Then, the data is split into training and testing sets. The pipeline is set up with two main components: CountVectorizer for converting text data into numerical vectors and Logistic Regression for classification. Various parameters for CountVectorizer, such as the maximum number of features, minimum document frequency, and n-gram range, are specified for optimization. GridSearchCV is employed to systematically search through these parameter combinations using 5-fold cross-validation. After fitting the pipeline to the training data, predictions are made on the testing data and the resulting performance metrics are calculated.</p>\n<pre>#Initalize Whitespace tokenizer<br>w_tokenizer = nltk.tokenize.WhitespaceTokenizer()<br>#Initalize WorkNet lemmatizer<br>lemmatizer = nltk.stem.WordNetLemmatizer()<br><br>#Define function to lemmatize text<br>def lemmatize_text(text):<br>    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])<br><br>#Apply lemmatization to predictive variables<br>X = df['answer'].apply(lemmatize_text)<br><br>#Train test split<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)<br><br>#Set up pipeline<br>pipe = Pipeline([<br>    ('cvec', CountVectorizer()),  # Vectorization<br>    ('lr', LogisticRegression(max_iter=1000)) #Raised max_iter as was getting an error that total no. of iterations reached limit<br>])<br><br>#Pipeline parameters<br>pipe_params = {<br>    'cvec__max_features': [None, 500, 1000, 5000],<br>    'cvec__min_df': [1, 10, 50],<br>    'cvec__max_df': [0.25, .5, 0.95],<br>    'cvec__ngram_range': [(1, 2), (1, 1)],<br>    'cvec__stop_words': ['english', None]<br>}<br><br>#Instantiate GridSearchCV<br>gs = GridSearchCV(pipe, #Object we are optimizing<br>                  pipe_params, #Parameters values for which we are searching<br>                  cv = 5) #5-fold cross-validation.<br><br>#Fit GridSearch to training data<br>gs.fit(X_train, y_train)<br><br>#Get predictions<br>preds = gs.predict(X_test)<br><br>#View confusion matrix<br>disp = ConfusionMatrixDisplay.from_estimator(gs, X_test, y_test, cmap='Reds', values_format='d');<br>disp.plot()<br><br>#Calculate normalized value counts of predicted values (compare to baseline)<br>y_pred_norm = pd.Series(preds).value_counts(normalize = True)<br><br>#Calculate parameters which result in highest score<br>best_param = gs.best_params_<br><br>#Calculate highest mean score<br>best_score = gs.best_score_<br><br>#Calculate score on training data<br>train_score = gs.score(X_train, y_train)<br><br>#Calculate score on testing data<br>test_score = gs.score(X_test, y_test)<br><br>print(y_pred_norm, best_param, best_score, train_score, test_score)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/507/1*AN2t4BifVFIK0K5OOXomlA.png\"><figcaption>Confusion Matrix for Model\u00a03</figcaption></figure><p>The values Model 3\u00a0gave:</p>\n<ul>\n<li>y_pred Human:\u00a055.3%</li>\n<li>y_pred ChatGPT:\u00a044.7%</li>\n<li>Best score:\u00a091.1%</li>\n<li>Train score:\u00a099.2%</li>\n<li>Test score:\u00a090.5%</li>\n</ul>\n<h3>Conclusion</h3>\n<p>The analysis of the classification models revealed that out of the three models evaluated, the final one emerged as the most effective in distinguishing between human-generated responses and those generated by ChatGPT. With a prediction accuracy of 90.5% on the testing data, the model demonstrated robust performance in correctly categorizing responses. Notably, the best score achieved during hyperparameter tuning reached 91.1%, underscoring the effectiveness of the chosen techniques in optimizing model performance. Furthermore, the model exhibited high consistency between training and testing scores, with minimal overfitting, as evidenced by the marginal difference between the two. These findings underscore the significance of employing natural language processing techniques coupled with classification modeling in discerning between human and bot-generated responses, offering valuable insights for maintaining the integrity of online conversations and enhancing the development of AI-driven content moderation tools.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=96b5e03ae188\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>In the ever-expanding digital landscape, the challenge of distinguishing between genuine human responses and those generated by bots has become increasingly pertinent. Leveraging the power of Natural Language Processing (NLP) and classification modeling, this project attempts to tackle this challenge head-on by developing an AI system capable of accurately discerning between bot-generated and human-crafted responses. By harnessing data from Reddit discussions and interactions with ChatGPT, the project will meticulously train and test the AI model, exploring the nuances and patterns that distinguish between the two types of responses. Through this exploration, we aim to not only shed light on the efficacy of AI in tackling this pressing issue but also contribute to the development of more robust tools for maintaining the integrity of online conversations. Join us on this journey as we delve into the intersection of AI, NLP, and the intricacies of human communication.</p>\n<p><strong>Part 1</strong> of this article focuses on <strong>Data Wrangling/Gathering/Acquisition</strong>. We will gather questions &amp; answer pairs from various subreddits, then ask that same question too ChatGPT and gather that answer as\u00a0well.</p>\n<p><strong>Part 2</strong> focuses on <strong>Data Cleaning &amp; EDA</strong>. We will clean our data, as well as perform elementary exploratory data analysis.</p>\n<p><strong>Part 3</strong> focuses on <strong>NLP (Natural Language Processing) &amp; Classification Modelling.</strong></p>\n<p>Below, I will include all the necessary imports needed for this project and a link to the github repo. Do note, all associated files can be found within the \u2018medium\u2019\u00a0folder.</p>\n<p><a href=\"https://github.com/bamarasingam/detect-chatgpt-vs-human-text/tree/main/medium\">detect-chatgpt-vs-human-text/medium at main \u00b7 bamarasingam/detect-chatgpt-vs-human-text</a></p>\n<pre>import pandas as pd<br>import numpy as np<br>import matplotlib.pyplot as plt<br>import nltk<br>from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score<br>from sklearn.pipeline import Pipeline<br>from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB<br>from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer<br>from sklearn.linear_model import LogisticRegression<br>from sklearn.metrics import classification_report<br>from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay<br>from nltk.corpus import stopwords<br>from nltk.stem import WordNetLemmatizer, PorterStemmer<br>from nltk.tokenize import word_tokenize</pre>\n<h3>Part 1: Data Wrangling/Gathering/Acquisition</h3>\n<p>Initially, we need to gather data to which we can analyze. I decided to gather my human responses from Reddit, and my bot responses from\u00a0ChatGPT.</p>\n<h4>Reddit Data\u00a0Gather</h4>\n<p>Firstly, we need to set up the Reddit API. I suggest to check the docs below, but essentially you need to make an account so that you can have the necessary items to call and use the API (free to\u00a0use).</p>\n<p><a href=\"https://praw.readthedocs.io/en/stable/getting_started/quick_start.html\">https://praw.readthedocs.io/en/stable/getting_started/quick_start.html</a></p>\n<pre>import praw<br>import pandas as pd #Importing for use later<br><br>reddit = praw.Reddit(<br>    client_id=\"\",<br>    client_secret=\"\",<br>    user_agent=\"dsi_project_3\",<br>)</pre>\n<p>Once filled in, we will need to collect various responses given a question. I chose subreddits predicated on posts which ask questions.</p>\n<pre>sr1 = reddit.subreddit('askculinary')<br>sr2 = reddit.subreddit('questions')<br>sr3 = reddit.subreddit('askengineers')<br>sr4 = reddit.subreddit('cscareerquestions')<br>sr5 = reddit.subreddit('askdocs')<br>sr10 = reddit.subreddit('TrueAskReddit')<br><br>subreddits = [sr1, sr2, sr3, sr4, sr5, sr6, sr7, sr8, sr9, sr10]</pre>\n<p>Next, we will set up a function to gather said questions, and the top response for that question from a given subreddit. It will put this information in a dictionary, with the question and answer being the key-value pair respectively. Finally, we will add all of these key-value pairs into a\u00a0list.</p>\n<pre>def question_and_answer(sr):<br>    top_questions = sr.top(limit = 625) #Limit to collecting only 625 questions<br>    q_and_a = []<br><br>    for submission in top_questions:<br>        if '?' in submission.title: #Only using posts with ? in it, to ensure questions are being used<br>            top_comment = submission.comments[0].body if submission.comments else None<br><br>        #Setting up dictionary<br>        question_data = { <br>            'title': submission.title,<br>            'top_comment': top_comment <br>        }<br><br>        q_and_a.append(question_data) #Adding key-value pairs too list</pre>\n<p>Apply this function for each subreddit, turn the list into a dataframe, and combined all of these dataframes into one. You should then have a dataframe consisting of Reddit question &amp;\u00a0answers.</p>\n<pre>#Applying function too each subreddit<br>q_and_a_sr1 = question_and_answer(sr1)<br>q_and_a_sr2 = question_and_answer(sr2)<br>q_and_a_sr3 = question_and_answer(sr3)<br>q_and_a_sr4 = question_and_answer(sr4)<br>q_and_a_sr5 = question_and_answer(sr5)<br>q_and_a_sr10 = question_and_answer(sr10)<br><br>#Turning each list into a datafframe<br>df_sr1 = pd.DataFrame(q_and_a_sr1)<br>df_sr2 = pd.DataFrame(q_and_a_sr2)<br>df_sr3 = pd.DataFrame(q_and_a_sr3)<br>df_sr4 = pd.DataFrame(q_and_a_sr4)<br>df_sr5 = pd.DataFrame(q_and_a_sr5)<br>df_sr10 = pd.DataFrame(q_and_a_sr10)<br><br>#Combine all the dataframes above<br>df_reddit = pd.concat([df_sr1, df_sr2, df_sr3, df_sr4, df_sr5, df_sr10], axis = 0)</pre>\n<p>Finally, we will export this into a csv file to be used\u00a0later.</p>\n<pre>df_reddit.to_csv('reddit_ask_question.csv', index = False)</pre>\n<h4>ChatGPT Data\u00a0Gather</h4>\n<p>For this part of the process, you will need to create an OpenAI account and gather an API Key which can be used to gather the necessary data. We will also read in our dataframe from the last step as they contain the questions which we will be asking\u00a0ChatGPT.</p>\n<pre>import openai<br>import pandas as pd<br><br>openai.api_key = ''<br>reddit = pd.read_csv('df_reddit.csv')</pre>\n<p>So one thing about OpenAI is they have various models which can be used to generate responses, each with it\u2019s own pricing model. I will include the steps and model I used, although there are various ways to go about this to which you will incur various costs associated too using the\u00a0API.</p>\n<p>First step is to set up a function which uses ChatGPT to pose a question, and gather the response from that question.</p>\n<pre>def chat_with_gpt3(question):<br>    response = openai.Completion.create(<br>        engine = \"text-davinci-003\", #OpenAI Model<br>        prompt = question, #Question to ask<br>        temperature = 0.6, #Randomness within answer (sampling temp)<br>        max_tokens = 500, #Adjust max_tokens based on the desired response length<br>        n = 20 #Number of questions in a batch<br><br>    )<br>    #The below line was taken from docs to extract the response<br>    return response['choices'][0]['text'] ##https://platform.openai.com/docs/guides/gpt/chat-completions-api</pre>\n<p>Next we will make a list of our questions taken from our dataframe, and run a loop to pose these questions to ChatGPT, and gather all of the responses in a separate list which we will call \u201canswers\u201d.</p>\n<pre># Get the questions from the 'title' column of the dataframe<br>questions = reddit['title'].tolist()<br><br># List to store the answers<br>answers = []<br><br># Loop through the questions and get answers<br>for question in questions:<br>    answer = chat_with_gpt3(question)<br>    answers.append(answer)</pre>\n<p>Once completed, we can now add these answers to our dataframe as our third column and export this dataframe into a csv file to be used\u00a0later.</p>\n<pre>reddit['answers'] = answers<br><br>reddit.to_csv('reddit_ai_df.csv', index = False)</pre>\n<p>At this point, your dataframe should look something like this with the appropriate headers for each\u00a0column:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*z3bSKhNjSfGYEzA79mgwPw.png\"></figure><p>Do note, in my repo I had to break this part into steps as I continually got errors from the server side. I ended up breaking my questions into 8 parts, to which I applied the function to each part, got my lists of responses and combined them all to then be added as the final column too our dataframe.</p>\n<h3>Part 2: Data Cleaning &amp;\u00a0EDA</h3>\n<p>When it comes to cleaning the data, there is a lot to account for. Most of the cleaning has to do with automated messages by Reddit itself, or the way answers would come in when collected from ChatGPT. I tried too include all the ones I could find, which consists\u00a0of:</p>\n<ul>\n<li>Need to remove rows which include [deleted] in\u00a0comment</li>\n<li>Need to remove rows which include \u201c#Message to all\u00a0users\u2026\u201d</li>\n<li>Delete rows which have [removed]</li>\n<li>Remove rows which start with \u201cWelcome\u201d</li>\n<li>Remove NaN</li>\n<li>Remove various ways people use markdown shortcuts in Reddit\u00a0comments</li>\n</ul>\n<p>I will include a summarized version of the code I used to clean my data, for a more detailed version please visit the repo on\u00a0GitHub.</p>\n<pre>#Read in csv file from previous part<br>real = pd.read_csv('reddit_ai_df.csv')<br><br>#Data Cleaning <br>real['answers'] = real['answers'].str.strip() #Removing white space from all ChatGPT answers<br>real = real.dropna() #Drop rows with NaN<br>real = real[real.top_comment != '[deleted]'] #Drop rows which include [deleted] in top comment<br>real = real[real.top_comment != '[removed]'] #Drop rows which include [removed] in top comment<br>real = real[real.top_comment != 'Welcome to r/TrueAskReddit. Remember that this subreddit is aimed at high quality discussion, so please elaborate on your answer as much as you can and avoid off-topic or jokey answers as per [subreddit rules](https://www.reddit.com/r/TrueAskReddit/about/sidebar).\\n\\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/TrueAskReddit) if you have any questions or concerns.*']<br>real['answers'] = real['answers'].str.replace('\\n\\n', ' ') #Delete line breaks from ChatGPT answers<br>real['top_comment'] = real['top_comment'].str.replace('\\n', ' ') #Delete line breaks from Reddit comments<br>real = real[real.top_comment != f\"A recent Reddit policy change threatens to kill many beloved third-party mobile apps, making a great many quality-of-life features not seen in the official mobile app **permanently inaccessible** to users.  On May 31, 2023, Reddit announced they were raising the price to make calls to their API from being free to a level that will kill every third party app on Reddit, from [Apollo](https://www.reddit.com/r/apolloapp/comments/13ws4w3/had_a_call_with_reddit_to_discuss_pricing_bad/) to [Reddit is Fun](https://www.reddit.com/r/redditisfun/comments/13wxepd/rif_dev_here_reddits_api_changes_will_likely_kill/) to [Narwhal](https://www.reddit.com/r/getnarwhal/comments/13wv038/reddit_have_quoted_the_apollo_devs_a_ridiculous/jmdqtyt/) to [BaconReader](https://www.reddit.com/r/baconreader/comments/13wveb2/reddit_api_changes_and_baconreader/).  Even if you're not a mobile user and don't use any of those apps, this is a step toward killing other ways of customizing Reddit, such as Reddit Enhancement Suite or the use of the old.reddit.com desktop interface .  This isn't only a problem on the user level: many subreddit moderators depend on tools only available outside the official app to keep their communities on-topic and spam-free.  &amp;#x200B;  What can *you* do?  1. **Complain.** Message the mods of r/reddit.com, who are the admins of the site: message [/u/reddit](https://www.reddit.com/u/reddit/): submit a [support request](https://support.reddithelp.com/hc/en-us/requests/new): comment in relevant threads on [r/reddit](https://www.reddit.com/r/reddit/), such as [this one](https://www.reddit.com/r/reddit/comments/12qwagm/an_update_regarding_reddits_api/), leave a negative review on their official iOS or Android app- and sign your username in support to this post. 2. **Spread the word.** Rabble-rouse on related subreddits. Meme it up, make it spicy. Bitch about it to your cat. Suggest anyone you know who moderates a subreddit join us at our sister sub at [r/ModCoord](https://www.reddit.com/r/ModCoord/) \\\\- but please don't pester mods you *don't* know by simply spamming their modmail. 3. **Boycott** ***and*** **spread the word...to Reddit's competition!** Stay off Reddit as much as you can, instead, take to your favorite *non*\\\\-Reddit platform of choice and make some noise in support!   https://discord.gg/cscareerhub  https://programming.dev  4. **Don't be a jerk.** As upsetting this may be, threats, profanity and vandalism will be worse than useless in getting people on our side. Please make every effort to be as restrained, polite, reasonable and law-abiding as possible.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/cscareerquestions) if you have any questions or concerns.*\"]<br>real['top_comment'] = real['top_comment'].str.replace('# ', ' ') #Deleting all # with a space afterwards as represents header</pre>\n<p>At this point, we can do a little EDA and see what words are being used the most. We will use the NLTK (Natural Language Toolkit) to help analyze our text, alongside CountVectorizer from the scikitlearn library.</p>\n<p>The block of code below performs text preprocessing and vectorization on the answers collected from Reddit, using the \u2018top_comment\u2019 column of our dataframe named \u201creal\u201d. It starts by initializing a CountVectorizer object, configured to exclude English stopwords (stopwords are common words which are excluded from analysis as they don\u2019t carry much semantic meaning). Then, it fits the CountVectorizer to the \u2018top_comment\u2019 column, learning the vocabulary of the text data. Next, it transforms the text data into a matrix representation, where each row represents a comment and each column represents a unique word in the vocabulary, with cell values indicating word counts. Finally, it converts the sparse matrix into a dataframe, creating a structured dataset suitable for further analysis or machine learning tasks. We will also plot the 30 most used words to see if we can notice anything of importance.</p>\n<pre>#Instantiate CVEC while excluding stop words<br>cvec = CountVectorizer(stop_words='english')<br>#Fit to column<br>cvec.fit(real['top_comment'])<br>#Transform column<br>top_comments = cvec.transform(real['top_comment'])<br>#Dataframe with transformed data<br>df_reddit_cvec = pd.DataFrame(top_comments.todense(), columns = cvec.get_feature_names_out())<br>df_reddit_cvec<br><br>#Set figure size<br>plt.figure(figsize=(15,7))<br>#Plotting 30 most used words<br>df_reddit_cvec.sum().sort_values(ascending=False).head(30).plot(kind='barh')<br>plt.title(\"Most Frequent Words from Reddit (Excluding Stop Words)\")<br>plt.xlabel('Frequency')<br>plt.ylabel('Word')</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*ISLwRa7Z1uPQi2jLfvmT3Q.png\"></figure><p>From the visual above, we can see there are certain words showing up which ChatGPT would never spit out such as www, com, askdocs, etc. Below we will create a loop to remove any rows which contain these keywords.</p>\n<pre># Define a list of keywords to check for in 'top_comment' column<br>keywords_to_exclude = ['www', 'https', 'com', 'subreddit', 'reddit', 'askdocs', 'askculinary', 'askengineers', 'cscareerquestions', 'TrueAskReddit']<br><br># Iterate through each keyword and remove rows containing it in 'top_comment'<br>for keyword in keywords_to_exclude:<br>    real = real[~real['top_comment'].str.contains(keyword)]<br># Reset index after removing rows<br>real.reset_index(drop=True, inplace=True)</pre>\n<p>I did the same for our ChatGPT answers, all located in the \u2018answers\u2019 column within the dataframe named \u201creal\u201d. However, no further cleaning was needed so I will leave this step out of the article for redundancy.</p>\n<p>The nature of this problem is a <strong>binary classification</strong> issue; either a response is given by a human, or by ChatGPT. Currently we have 3 columns in our dataframe consisting of our question, a human answer, and ChatGPT\u2019s answer.</p>\n<p>We will have to create a new dataframe, with all of the answers in one column, and the second column explicating who the answer is from. We will use 0 for human answers, and 1 for ChatGPT\u00a0answers.</p>\n<pre>#New dataframe with answers from human, and 0 to represent human response<br>df1 = pd.DataFrame()<br>df1['answer'] = real['top_comment']<br>df1['who_from'] = 0<br><br>#New dataframe with answers from AI, and 1 to represent AI response<br>df2 = pd.DataFrame()<br>df2['answer'] = real['answers']<br>df2['who_from'] = 1<br><br>#Put df1 and df2 together<br>df = pd.concat([df1, df2])<br>df = df.reset_index()<br>df = df.dropna()</pre>\n<p>Once complete, you should have something which looks like\u00a0this:</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/930/1*Gi9Uot7n7XNInp_RCj4kcw.png\"></figure><h3>Part 3: NLP (Natural Language Processing) &amp; Classification Modelling.</h3>\n<p>When it comes to NLP &amp; Classification models, there are a myriad of ways too which you can go about it. Since we are working with a binary classification problem, I\u2019ve decided to focus on Log Regression and Bernoulli Naive Bayes (BNB) as our choice for modelling. I will talk over 3 different methods in this article, but you can find a total of 8 within the repo if you would like to see/explore more.</p>\n<h4>Baseline &amp; Model Specifications</h4>\n<p>Firstly we will figure out the baseline for our model. Setting a baseline provides a point of reference for evaluating the performance of our machine learning models. Understanding the baseline performance helps in determining whether the developed models offer significant improvements over simple, naive approaches and guides the selection of appropriate algorithms and feature engineering strategies.</p>\n<p>We will do this by first assigning the data into either predictive (X) variable, or target (Y) variable. The \u2018answer\u2019 column will be our predictive variable (what we use too make predictions) while the \u2018who_from\u2019 column will be our target (what we are trying to predict).</p>\n<p>We\u2019ll also have to split our data into training and testing sets, and get a baseline for our y_test set to compare to when we make our predictions.</p>\n<pre>#Split data into predictive variables and target variable<br>X = df['answer']<br>y = df['who_from']<br><br>#Baseline<br>y.value_counts(normalize = True)<br><br>#Train test split, test size 20% <br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)<br><br>#y_test baseline<br>y_test.value_counts(normalize = True)</pre>\n<p>Our baseline for y is 50% Reddit responses and 50% ChatGPT responses. For our y_test, our baseline is <strong>52.9% for Reddit</strong>, and <strong>47.1% for\u00a0ChatGPT</strong>.</p>\n<p>For our models, we will be using either Logistic Regression or Bernoulli Naive Bayes, mixed in with either Count Vectorization or TFIDF Vectorization (Terms Frequency Inverse Document Frequency Vectorization). Here\u2019s a short definition for\u00a0each:</p>\n<ul>\n<li>Logistic Regression: A linear classification algorithm used to model the probability of a binary outcome based on one or more predictor variables.</li>\n<li>Bernoulli Naive Bayes: A variant of the Naive Bayes algorithm suitable for binary feature vectors, often used for text classification tasks.</li>\n<li>Count Vectorization: A method of converting text data into numerical vectors by counting the frequency of each word in a document, typically used for bag-of-words models.</li>\n<li>TFIDF Vectorization: A method of converting text data into numerical vectors by weighting the term frequency (TF) by the inverse document frequency (IDF), aiming to highlight words that are important to individual documents but not common across the entire\u00a0corpus.</li>\n</ul>\n<h4>Model 1: Log Regression + TFIDF Vectorization</h4>\n<p>For our first model, we will implement a text classification task using the TFIDF (Term Frequency-Inverse Document Frequency) vectorization technique and a logistic regression model. First, we initialize a TFIDF vectorizer to convert the text data into numerical vectors. The training data is then transformed using this vectorizer, while the testing data is transformed using the same vectorizer to maintain consistency. Next, we instantiate a logistic regression model and fit it to the training data. After training, we predict the target variable for the testing data and evaluate the model\u2019s performance using a confusion matrix, which provides insights into the model\u2019s ability to correctly classify instances. Additionally, we calculate the accuracy scores of the model on both the training and testing data to assess its overall performance. Finally, we visualize the confusion matrix to gain a better understanding of the model\u2019s strengths and weaknesses in classifying the\u00a0data.</p>\n<pre>#Instantiate TFIDF vectorizer<br>tfidf = TfidfVectorizer()<br><br>#Fit and transforming training data<br>X_train_tfidf = tfidf.fit_transform(X_train)<br><br>#Transform testing data using same TFIDF Vectorizer<br>X_test_tfidf = tfidf.transform(X_test)<br><br>#Instantiate Log Regression model<br>lr = LogisticRegression()<br><br>#Fit Log Regression model to training data<br>lr.fit(X_train_tfidf, y_train)<br><br>#Predict target variable for testing data<br>y_pred = lr.predict(X_test_tfidf)<br><br>#Calculate confusion matrix using actual and predicted values<br>cm = confusion_matrix(y_test, y_pred)<br><br>#Create and display confusion matrix plot<br>disp = ConfusionMatrixDisplay(confusion_matrix = cm)<br>disp.plot()<br><br>#Calculate normalized value counts of predicted values (compare to baseline)<br>y_pred_norm = pd.Series(y_pred).value_counts(normalize = True)<br><br>#Calculate accuracy score on training data<br>train_score = lr.score(X_train_tfidf, y_train)<br><br>#Calculate accuracy score on testing data<br>test_score = lr.score(X_test_tfidf, y_test)<br><br>print(y_pred_norm, train_score, test_score)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/507/1*y0S4muqdK-8rbnQc0ASXyA.png\"><figcaption>Confusion Matrix for Model\u00a01</figcaption></figure><p>The values which Model 1 gave in relation to the last 3 lines of\u00a0code:</p>\n<ul>\n<li>y_pred human:\u00a053.2%</li>\n<li>y_pred ChatGPT:\u00a046.8%</li>\n<li>Train score:\u00a095.0%</li>\n<li>Test score:\u00a089.0%</li>\n</ul>\n<h4>Model 2: Bernouli Naive Bayes + Count Vectorization using Pipeline &amp; GridSearch w/Stop\u00a0Words</h4>\n<p>The second model starts by setting up a pipeline. A pipeline is a sequence of data processing steps that are chained together to automate workflow. In this pipeline, we use CountVectorizer to convert text data into numerical feature vectors and the Bernoulli Naive Bayes classifier for classification. We then define a set of parameters to be tuned using GridSearchCV, a technique that exhaustively searches through a specified parameter grid to find the best combination of hyperparameters. Through 5-fold cross-validation, the model is trained on various subsets of the training data to ensure robustness. Once the best model is identified, we make predictions on the test data and evaluate its performance.</p>\n<pre>#Set up pipeline<br>pipe = Pipeline([<br>    ('cvec', CountVectorizer()),<br>    ('bnb', BernoulliNB())<br>])<br><br>#Setting up (English) stop words<br>nltk_stop = stopwords.words('english')<br><br>#Pipeline parameters<br>pipe_params = {<br>    'cvec__max_features' : [None, 1000], # Maximum number of features fit<br>    'cvec__min_df' : [1, 5, 10], # Minimum number of documents needed to include token<br>    'cvec__max_df' : [0.9, .95], # Maximum number of documents needed to include token<br>    'cvec__ngram_range' : [(1, 2), (1,1)], #Check (individual tokens) and also check (individual tokens and 2-grams)<br>    'cvec__stop_words' : ['english', None, nltk_stop] #Words to remove from text data<br>}<br><br>#Instantiate GridSearchCV.<br>gs = GridSearchCV(pipe, #Object we are optimizing<br>                  pipe_params, #Parameters values for which we are searching<br>                  cv = 5) #5-fold cross-validation<br><br>#Fit GridSearch to training data.<br>gs.fit(X_train, y_train)<br><br>#Get predictions<br>preds = gs.predict(X_test)<br><br>#View confusion matrix<br>disp = ConfusionMatrixDisplay.from_estimator(gs, X_test, y_test, cmap='Reds', values_format='d');<br>disp.plot()<br><br>#Calculate normalized value counts of predicted values (compare to baseline)<br>y_pred_norm = pd.Series(preds).value_counts(normalize = True)<br><br>#Calculate parameters which result in highest score<br>best_param = gs.best_params_<br><br>#Calculate highest mean score<br>best_score = gs.best_score_<br><br>#Calculate score on training data<br>train_score = gs.score(X_train, y_train)<br><br>#Calculate score on testing data<br>test_score = gs.score(X_test, y_test)<br><br>print(y_pred_norm, best_param, best_score, train_score, test_score)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/507/1*p0Rw8hRTTYcpOdhUYKwu6A.png\"><figcaption>Confusion Matrix for Model\u00a02</figcaption></figure><p>The values Model 2\u00a0gave:</p>\n<ul>\n<li>y_pred Human:\u00a061.1%</li>\n<li>y_pred ChatGPT:\u00a038.9%</li>\n<li>Best score:\u00a087.5%</li>\n<li>Train score:\u00a090.8%</li>\n<li>Test score:\u00a087.8%</li>\n</ul>\n<h4>Model 3: Log Regression + Count Vectorization using Pipeline &amp; GridSearch w/Lemmatization</h4>\n<p>Our final model implements a pipeline for text classification using logistic regression with hyperparameter tuning through grid search. Initially, the text data undergoes lemmatization, a process that reduces words to their base or dictionary form (lemma). This helps in standardizing word with different forms to their common base form. Then, the data is split into training and testing sets. The pipeline is set up with two main components: CountVectorizer for converting text data into numerical vectors and Logistic Regression for classification. Various parameters for CountVectorizer, such as the maximum number of features, minimum document frequency, and n-gram range, are specified for optimization. GridSearchCV is employed to systematically search through these parameter combinations using 5-fold cross-validation. After fitting the pipeline to the training data, predictions are made on the testing data and the resulting performance metrics are calculated.</p>\n<pre>#Initalize Whitespace tokenizer<br>w_tokenizer = nltk.tokenize.WhitespaceTokenizer()<br>#Initalize WorkNet lemmatizer<br>lemmatizer = nltk.stem.WordNetLemmatizer()<br><br>#Define function to lemmatize text<br>def lemmatize_text(text):<br>    return ' '.join([lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)])<br><br>#Apply lemmatization to predictive variables<br>X = df['answer'].apply(lemmatize_text)<br><br>#Train test split<br>X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)<br><br>#Set up pipeline<br>pipe = Pipeline([<br>    ('cvec', CountVectorizer()),  # Vectorization<br>    ('lr', LogisticRegression(max_iter=1000)) #Raised max_iter as was getting an error that total no. of iterations reached limit<br>])<br><br>#Pipeline parameters<br>pipe_params = {<br>    'cvec__max_features': [None, 500, 1000, 5000],<br>    'cvec__min_df': [1, 10, 50],<br>    'cvec__max_df': [0.25, .5, 0.95],<br>    'cvec__ngram_range': [(1, 2), (1, 1)],<br>    'cvec__stop_words': ['english', None]<br>}<br><br>#Instantiate GridSearchCV<br>gs = GridSearchCV(pipe, #Object we are optimizing<br>                  pipe_params, #Parameters values for which we are searching<br>                  cv = 5) #5-fold cross-validation.<br><br>#Fit GridSearch to training data<br>gs.fit(X_train, y_train)<br><br>#Get predictions<br>preds = gs.predict(X_test)<br><br>#View confusion matrix<br>disp = ConfusionMatrixDisplay.from_estimator(gs, X_test, y_test, cmap='Reds', values_format='d');<br>disp.plot()<br><br>#Calculate normalized value counts of predicted values (compare to baseline)<br>y_pred_norm = pd.Series(preds).value_counts(normalize = True)<br><br>#Calculate parameters which result in highest score<br>best_param = gs.best_params_<br><br>#Calculate highest mean score<br>best_score = gs.best_score_<br><br>#Calculate score on training data<br>train_score = gs.score(X_train, y_train)<br><br>#Calculate score on testing data<br>test_score = gs.score(X_test, y_test)<br><br>print(y_pred_norm, best_param, best_score, train_score, test_score)</pre>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/507/1*AN2t4BifVFIK0K5OOXomlA.png\"><figcaption>Confusion Matrix for Model\u00a03</figcaption></figure><p>The values Model 3\u00a0gave:</p>\n<ul>\n<li>y_pred Human:\u00a055.3%</li>\n<li>y_pred ChatGPT:\u00a044.7%</li>\n<li>Best score:\u00a091.1%</li>\n<li>Train score:\u00a099.2%</li>\n<li>Test score:\u00a090.5%</li>\n</ul>\n<h3>Conclusion</h3>\n<p>The analysis of the classification models revealed that out of the three models evaluated, the final one emerged as the most effective in distinguishing between human-generated responses and those generated by ChatGPT. With a prediction accuracy of 90.5% on the testing data, the model demonstrated robust performance in correctly categorizing responses. Notably, the best score achieved during hyperparameter tuning reached 91.1%, underscoring the effectiveness of the chosen techniques in optimizing model performance. Furthermore, the model exhibited high consistency between training and testing scores, with minimal overfitting, as evidenced by the marginal difference between the two. These findings underscore the significance of employing natural language processing techniques coupled with classification modeling in discerning between human and bot-generated responses, offering valuable insights for maintaining the integrity of online conversations and enhancing the development of AI-driven content moderation tools.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=96b5e03ae188\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["data-science","ai","machine-learning","nlp","binary-classification"]}]}